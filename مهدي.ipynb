{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyO7xRtHfBvNN6qViGOE3Lc3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahdi-alalawi/AndroidTutorialForBeginners/blob/master/%D9%85%D9%87%D8%AF%D9%8A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar xzf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "%cd .."
      ],
      "metadata": {
        "id": "QgOS4h19oGSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ta-lib"
      ],
      "metadata": {
        "id": "vgvCow0doR2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "PxC8joqYVuu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n"
      ],
      "metadata": {
        "id": "syhicvANOu3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: انشئ كود يقراء البيانات من ملفcsv من Google Drive باسم gold_usd_prices\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the file path in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # Replace with your actual file path\n",
        "\n",
        "try:\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Now you can work with the DataFrame 'df'\n",
        "  print(df.head())  # Print the first few rows to verify\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "  print(f\"Error: Could not parse the CSV file at {file_path}. Check the file format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "FEx_Q0oNPEsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# تنظيف البيانات\n",
        "def clean_data(data):\n",
        "    \"\"\"تنظيف البيانات وإزالة القيم المفقودة\"\"\"\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "68iEQLeEP-I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the file path in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # Replace with your actual file path\n",
        "\n",
        "try:\n",
        "  # Read the CSV file into a pandas DataFrame\n",
        "  df = pd.read_csv(file_path)\n",
        "\n",
        "  # Now you can work with the DataFrame 'df'\n",
        "  print(df.head())  # Print the first few rows to verify\n",
        "\n",
        "  # تنظيف البيانات\n",
        "  def clean_data(data):\n",
        "      \"\"\"تنظيف البيانات وإزالة القيم المفقودة\"\"\"\n",
        "      data.dropna(inplace=True)\n",
        "      return data\n",
        "\n",
        "  df = clean_data(df) # Apply the cleaning function\n",
        "  print(df.head()) # Print cleaned data\n",
        "\n",
        "except FileNotFoundError:\n",
        "  print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "  print(f\"Error: Could not parse the CSV file at {file_path}. Check the file format.\")\n",
        "except Exception as e:\n",
        "  print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "z5tH2_53Swqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: انشئ كود يقوم بقرائة عناوين المفكرة الاقتصادية لموقع investingو يستعرض ويقارن صدور الخبر الحالي و التقدير و السابق و يقوم باستدعاء الاخبار الاعلى تاثيرا على الدولار الامريكي و الذهب\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def get_investing_economic_calendar():\n",
        "    \"\"\"\n",
        "    يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # تحديد العناصر التي تحتوي على عناوين الأخبار\n",
        "        news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "\n",
        "        news_data = []\n",
        "\n",
        "        for news_element in news_elements:\n",
        "            title_element = news_element.select_one(\"td.left.event\")\n",
        "\n",
        "            if title_element:\n",
        "                title = title_element.text.strip()\n",
        "                # استخراج باقي البيانات (الوقت، التأثير المتوقع، السابق، الحالي)  -  يجب تعديل selectors حسب بنية موقع Investing.com\n",
        "                time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "\n",
        "                previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "\n",
        "                current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "\n",
        "\n",
        "                news_data.append({\n",
        "                    \"title\": title,\n",
        "                    \"time\": time,\n",
        "                    \"impact\": impact,\n",
        "                    \"previous\": previous,\n",
        "                    \"current\": current\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(news_data)\n",
        "        print(df)\n",
        "\n",
        "        # إضافة عملية مقارنة بين القيم الحالية والسابقة والتقدير\n",
        "        # ...\n",
        "        # استدعاء الأخبار الأعلى تأثيرًا على الدولار الأمريكي والذهب\n",
        "        # ...\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "# استدعاء الدالة\n",
        "get_investing_economic_calendar()"
      ],
      "metadata": {
        "id": "V7gh1klWUF7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد مع تعليق باللغة العربية\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # طباعة أولى الصفوف للتحقق\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتنظيف البيانات\n",
        "    def clean_data(data):\n",
        "        \"\"\"\n",
        "        تنظيف البيانات وإزالة القيم المفقودة.\n",
        "        \"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    # تطبيق دالة التنظيف\n",
        "    df = clean_data(df)\n",
        "    print(df.head())  # طباعة البيانات بعد التنظيف\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data) # حفظ البيانات في متغير df_news\n",
        "            print(df_news)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "    get_investing_economic_calendar() # استدعاء الدالة\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"خطأ: الملف غير موجود في المسار {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"خطأ: تعذر تحليل ملف CSV في المسار {file_path}. تحقق من تنسيق الملف.\")\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ غير متوقع: {e}\")"
      ],
      "metadata": {
        "id": "UmbC749uXdML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد مع تعليق باللغة العربية\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # طباعة أولى الصفوف للتحقق\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتنظيف البيانات\n",
        "    def clean_data(data):\n",
        "        \"\"\"\n",
        "        تنظيف البيانات وإزالة القيم المفقودة.\n",
        "        \"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    # تطبيق دالة التنظيف\n",
        "    df = clean_data(df)\n",
        "    print(df.head())  # طباعة البيانات بعد التنظيف\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data) # حفظ البيانات في متغير df_news\n",
        "            print(df_news)\n",
        "            return df_news # Return the DataFrame\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar() # استدعاء الدالة وحفظ القيمة المعادة\n",
        "\n",
        "\n",
        "    # prompt: انشئ كود تحليل المشاعر من الأخبار باستخدام TextBlob\n",
        "\n",
        "    def analyze_news_sentiment(df_news):\n",
        "        \"\"\"\n",
        "        يقوم بتحليل مشاعر الأخبار الاقتصادية باستخدام TextBlob.\n",
        "        \"\"\"\n",
        "        if 'title' not in df_news.columns:\n",
        "            print(\"Error: 'title' column not found in the DataFrame.\")\n",
        "            return\n",
        "\n",
        "        df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "        return df_news\n",
        "\n",
        "    # استدعاء الدالة لتحليل مشاعر الأخبار\n",
        "    if df_news is not None: # Check if df_news was created successfully\n",
        "        df_news_with_sentiment = analyze_news_sentiment(df_news)\n",
        "        # طباعة الجدول مع مشاعر الأخبار\n",
        "        print(df_news_with_sentiment)\n",
        "    else:\n",
        "        print(\"Error: Could not retrieve news data.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"خطأ: الملف غير موجود في المسار {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"خطأ: تعذر تحليل ملف CSV في المسار {file_path}. تحقق من تنسيق الملف.\")\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ غير متوقع: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "rzMUeLnkYYLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد مع تعليق باللغة العربية\n",
        "\n",
        "# استيراد المكتبات اللازمة\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # طباعة أولى الصفوف للتحقق\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتنظيف البيانات\n",
        "    def clean_data(data):\n",
        "        \"\"\"\n",
        "        تنظيف البيانات وإزالة القيم المفقودة.\n",
        "        \"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    # تطبيق دالة التنظيف\n",
        "    df = clean_data(df)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())  # طباعة البيانات بعد التنظيف\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"خطأ: الملف غير موجود في المسار {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"خطأ: تعذر تحليل ملف CSV في المسار {file_path}. تحقق من تنسيق الملف.\")\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ غير متوقع: {e}\")"
      ],
      "metadata": {
        "id": "SMrC7dSnYjng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# استيراد المكتبات اللازمة\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # طباعة أولى الصفوف للتحقق\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتنظيف البيانات\n",
        "    def clean_data(data):\n",
        "        \"\"\"\n",
        "        تنظيف البيانات وإزالة القيم المفقودة.\n",
        "        \"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    # تطبيق دالة التنظيف\n",
        "    df = clean_data(df)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())  # طباعة البيانات بعد التنظيف\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"\n",
        "        Calculates order blocks based on price swings.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame with price data.\n",
        "            price_column: Name of the price column.\n",
        "            lookback: Number of periods to look back for swing highs and lows.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with added order block columns (buy_ob, sell_ob).\n",
        "        \"\"\"\n",
        "        # Convert the price column to numeric, handling errors\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        # Drop rows with missing values after conversion\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        # Find Buy Order Blocks (OBs)\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "\n",
        "        # Find Sell Order Blocks (OBs)\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Example usage assuming 'df' is your DataFrame with a 'Close' price column\n",
        "    # Make sure your data is loaded and cleaned correctly before running this code\n",
        "    if 'df' in locals() and not df.empty:  # Check if df exists and is not empty\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty. Please load the data correctly.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"خطأ: الملف غير موجود في المسار {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"خطأ: تعذر تحليل ملف CSV في المسار {file_path}. تحقق من تنسيق الملف.\")\n",
        "except Exception as e:\n",
        "    print(f\"حدث خطأ غير متوقع: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "59EzutLnaA3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# حساب Break of Structure (BOS)\n",
        "def calculate_bos(data):\n",
        "    \"\"\"حساب مناطق BOS\"\"\"\n",
        "    bos = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "            bos.append(data.index[i])\n",
        "    return bos\n"
      ],
      "metadata": {
        "id": "AkIKnS62eV5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجمع الاكواد و اجعلها كود واحد\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        Retrieves economic calendar data from Investing.com and performs sentiment analysis.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # Sentiment analysis\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "4-nCxP-ee0Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "   drive.mount('/content/drive')\n",
        "   # File path in Google Drive\n",
        "   file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "       # ... (Code for data loading, cleaning, and analysis) ...\n",
        "   except FileNotFoundError:\n",
        "       print(f\"Error: File not found at {file_path}\")\n",
        "   except pd.errors.ParserError:\n",
        "       print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "   except Exception as e:\n",
        "       print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Read CSV into DataFrame\n",
        "   df = pd.read_csv(file_path)\n",
        "   print(\"First 5 rows of the DataFrame:\")\n",
        "   print(df.head())\n",
        "\n",
        "   # Clean data (remove missing values)\n",
        "   df.dropna(inplace=True)\n",
        "   print(\"\\nDataFrame after cleaning:\")\n",
        "   print(df.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jjoae539nkvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import gdown\n",
        "\n",
        "def load_data_from_drive(file_id):\n",
        "    \"\"\"\n",
        "    تحميل البيانات من Google Drive باستخدام gdown.\n",
        "    \"\"\"\n",
        "    # تحميل الملف من Google Drive\n",
        "    url = f\"https://drive.google.com/uc?id={file_id}&export=download\"\n",
        "    gdown.download(url, 'gold_data.csv', quiet=False)\n",
        "\n",
        "    # قراءة ملف CSV\n",
        "    data = pd.read_csv('gold_data.csv', index_col='date', parse_dates=True)\n",
        "    return data\n",
        "\n",
        "    file_id = 'YOUR_FILE_ID'\n",
        "    data = load_data_from_drive(file_id)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GPFtrtURl_r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد مع التعليق باللغة العربية\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # ... (كود جلب البيانات وتحليل المشاعر كما هو)\n",
        "            pass # Placeholder for the existing code\n",
        "        except:\n",
        "            # ... (معالجة الأخطاء كما هو)\n",
        "            pass # Placeholder for the existing code\n",
        "\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        # ... (كود حساب order blocks كما هو)\n",
        "        pass # Placeholder for the existing code\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        # ... (كود حساب BOS كما هو)\n",
        "        pass # Placeholder for the existing code\n",
        "\n",
        "    # حساب Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "VZgZABFqg73n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# حساب مناطق العرض والطلب\n",
        "def calculate_supply_demand(data):\n",
        "    \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "    supply_zones = []\n",
        "    demand_zones = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['High'][i-1]:\n",
        "            supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "        elif data['Close'][i] < data['Low'][i-1]:\n",
        "            demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "    return supply_zones, demand_zones\n"
      ],
      "metadata": {
        "id": "7sDIqRDShRgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد و التعليق باللغة العربية\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # حساب Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i-1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "            elif data['Close'][i] < data['Low'][i-1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b0FauVC8hiLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# حساب خطوط الدعم والمقاومة\n",
        "def calculate_support_resistance(data):\n",
        "    \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "    support = data['Low'].rolling(window=20).min()\n",
        "    resistance = data['High'].rolling(window=20).max()\n",
        "    return support, resistance\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1jpRdFv2k8qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ادمج الاكواد و اجعلها كود واحد\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i-1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "            elif data['Close'][i] < data['Low'][i-1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "RtaNFP74lHQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# حساب مستويات فيبوناتشي\n",
        "def fibonacci_retracement(high, low):\n",
        "    \"\"\"حساب مستويات فيبوناتشي\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n"
      ],
      "metadata": {
        "id": "B9eYX1eEmRAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: اجعل الاكواد السابقة كود واحد مع التعليقات باللغة العربية\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i-1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "            elif data['Close'][i] < data['Low'][i-1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "DXKaKyi0mdoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# حساب مؤشر القوة النسبية (RSI)\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"حساب RSI\"\"\"\n",
        "    data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "    return data"
      ],
      "metadata": {
        "id": "9XTSk5YjnHSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i-1] and df['high_min'][i] > df['low_max'][i+1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i-1] and df['low_max'][i] < df['high_min'][i+1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i-1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "            elif data['Close'][i] < data['Low'][i-1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gCUkHWXEo1ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# حساب المتوسط المتحرك\n",
        "def calculate_moving_average(data, period=20):\n",
        "    \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "    data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "    return data"
      ],
      "metadata": {
        "id": "E5wIWSnOo7N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "a1Ev5VsSqCyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# حساب الزخم (Momentum)\n",
        "def calculate_momentum(data, period=10):\n",
        "    \"\"\"حساب الزخم\"\"\"\n",
        "    data['Momentum'] = data['Close'].diff(periods=period)\n",
        "    return data\n",
        "\n",
        "# تحليل حجم التداول\n",
        "def analyze_volume(data):\n",
        "    \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "gL0yOfZpqS0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب الزخم (Momentum)\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "        return data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "PQvH9VU2q_po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# حساب موجة الذئب\n",
        "def calculate_wolf_wave(data):\n",
        "    \"\"\"حساب موجة الذئب\"\"\"\n",
        "    wave_points = []\n",
        "    for i in range(1, len(data)-4):\n",
        "        if (data['Close'][i] < data['Close'][i-1] and\n",
        "            data['Close'][i+1] < data['Close'][i] and\n",
        "            data['Close'][i+2] > data['Close'][i+1] and\n",
        "            data['Close'][i+3] < data['Close'][i+2] and\n",
        "            data['Close'][i+4] > data['Close'][i+3]):\n",
        "            wave_points.append(data.index[i])\n",
        "    return wave_points\n"
      ],
      "metadata": {
        "id": "m9vk0Tp8rGRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب الزخم (Momentum)\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    def calculate_wolf_wave(data):\n",
        "        \"\"\"حساب موجة الذئب\"\"\"\n",
        "        wave_points = []\n",
        "        for i in range(1, len(data)-4):\n",
        "            if (data['Close'][i] < data['Close'][i-1] and\n",
        "                data['Close'][i+1] < data['Close'][i] and\n",
        "                data['Close'][i+2] > data['Close'][i+1] and\n",
        "                data['Close'][i+3] < data['Close'][i+2] and\n",
        "                data['Close'][i+4] > data['Close'][i+3]):\n",
        "                wave_points.append(data.index[i])\n",
        "        return wave_points\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "x1uF-UIcrscU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "إنشاء ميزات إضافية\n",
        "def create_features(data):\n",
        "    \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "    data['Price_Change'] = data['Close'].pct_change()\n",
        "    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "    return data\n",
        "\n",
        "# حساب مؤشر القنوات السعرية\n",
        "def calculate_price_channel(data, window=20):\n",
        "    \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "    data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "    data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "    data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "    return data"
      ],
      "metadata": {
        "id": "nWydNf8-sC0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب الزخم (Momentum)\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    def calculate_wolf_wave(data):\n",
        "        \"\"\"حساب موجة الذئب\"\"\"\n",
        "        wave_points = []\n",
        "        for i in range(1, len(data)-4):\n",
        "            if (data['Close'][i] < data['Close'][i-1] and\n",
        "                data['Close'][i+1] < data['Close'][i] and\n",
        "                data['Close'][i+2] > data['Close'][i+1] and\n",
        "                data['Close'][i+3] < data['Close'][i+2] and\n",
        "                data['Close'][i+4] > data['Close'][i+3]):\n",
        "                wave_points.append(data.index[i])\n",
        "        return wave_points\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    def create_features(data):\n",
        "        \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "        data['Price_Change'] = data['Close'].pct_change()\n",
        "        data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "        data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "        return data\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    def calculate_price_channel(data, window=20):\n",
        "        \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "        data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "        data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "        data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "        return data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0DuM157Dsrmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "def advanced_risk_management(data):\n",
        "    \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "    # حساب ATR (Average True Range)\n",
        "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "\n",
        "    # تحديد مستويات وقف الخسارة\n",
        "    data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "    data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "id": "UUu7c7sbs6Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # دالة لحساب مستويات فيبوناتشي\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب الزخم (Momentum)\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "        return data\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    def calculate_wolf_wave(data):\n",
        "        \"\"\"حساب موجة الذئب\"\"\"\n",
        "        wave_points = []\n",
        "        for i in range(1, len(data)-4):\n",
        "            if (data['Close'][i] < data['Close'][i-1] and\n",
        "                data['Close'][i+1] < data['Close'][i] and\n",
        "                data['Close'][i+2] > data['Close'][i+1] and\n",
        "                data['Close'][i+3] < data['Close'][i+2] and\n",
        "                data['Close'][i+4] > data['Close'][i+3]):\n",
        "                wave_points.append(data.index[i])\n",
        "        return wave_points\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    def create_features(data):\n",
        "        \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "        data['Price_Change'] = data['Close'].pct_change()\n",
        "        data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "        data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "        return data\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    def calculate_price_channel(data, window=20):\n",
        "        \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "        data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "        data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "        data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "        return data\n",
        "\n",
        "    # تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "    def advanced_risk_management(data):\n",
        "        \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "        # حساب ATR (Average True Range)\n",
        "        data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "\n",
        "        # تحديد مستويات وقف الخسارة\n",
        "        data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "        data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "        return data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "AEmEESn3tuL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# بناء الاستراتيجية\n",
        "def build_strategy(data):\n",
        "    \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "    data = clean_data(data)\n",
        "\n",
        "    # حساب Order Blocks وBOS وCHOCH\n",
        "    order_blocks = calculate_order_blocks(data)\n",
        "    bos = calculate_bos(data)\n",
        "    choch = calculate_choch(data)\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    data['Support'] = support\n",
        "    data['Resistance'] = resistance\n",
        "\n",
        "    # حساب مستويات فيبوناتشي\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "\n",
        "    # حساب RSI\n",
        "    data = calculate_rsi(data)\n",
        "\n",
        "    # حساب Moving Average\n",
        "    data = calculate_moving_average(data)\n",
        "\n",
        "    # حساب Momentum\n",
        "    data = calculate_momentum(data)\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    data = analyze_volume(data)\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    data = create_features(data)\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    data = calculate_price_channel(data)\n",
        "\n",
        "    # تحسين إدارة المخاطر\n",
        "    data = advanced_risk_management(data)\n"
      ],
      "metadata": {
        "id": "agWStOGykmKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            # ... (كود جلب البيانات وتحليل المشاعر كما هو)\n",
        "            pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "        except:\n",
        "            # ... (معالجة الأخطاء كما هو)\n",
        "            pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        # ... (كود حساب Order Blocks كما هو)\n",
        "        pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        # ... (كود حساب BOS كما هو)\n",
        "        pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        # ... (كود حساب CHOCH كما هو)\n",
        "        pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        # ... (كود حساب مناطق العرض والطلب كما هو)\n",
        "        pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        # ... (كود حساب خطوط الدعم والمقاومة كما هو)\n",
        "        pass  # تم إزالة الكود لتوفير المساحة، يمكنك إضافته هنا\n",
        "\n",
        "    # ... (باقي الدوال المساعدة كما هي)\n",
        "\n",
        "    # بناء الاستراتيجية\n",
        "    def build_strategy(data):\n",
        "        \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "        data = clean_data(data)  # تنظيف البيانات\n",
        "\n",
        "        # حساب Order Blocks وBOS وCHOCH\n",
        "        order_blocks = calculate_order_blocks(data)\n",
        "        bos = calculate_bos(data)\n",
        "        choch = calculate_choch(data)\n",
        "\n",
        "        # حساب مناطق العرض والطلب\n",
        "        supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "        # حساب خطوط الدعم والمقاومة\n",
        "        support, resistance = calculate_support_resistance(data)\n",
        "        data['Support'] = support\n",
        "        data['Resistance'] = resistance\n",
        "\n",
        "        # ... (باقي كود بناء الاستراتيجية كما هو)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U-qjspvMrMXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # دالة لتحليل مشاعر الأخبار الاقتصادية من موقع Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # دالة لحساب كتل الأوامر (Order Blocks)\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "\n",
        "        # رسم بياني لكتل الأوامر\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(df.index, df['Close'], label='Price')\n",
        "        plt.scatter(df.index[df['buy_ob'] != 0], df['buy_ob'][df['buy_ob'] != 0], marker='^', color='green', label='Buy Order Block')\n",
        "        plt.scatter(df.index[df['sell_ob'] != 0], df['sell_ob'][df['sell_ob'] != 0], marker='v', color='red', label='Sell Order Block')\n",
        "        plt.title('Order Blocks')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # دالة لحساب مناطق بداية التغيير (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "\n",
        "        # رسم بياني لمناطق BOS\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(data.index, data['Close'], label='Price')\n",
        "        plt.scatter(bos, data['Close'][bos], marker='o', color='blue', label='BOS')\n",
        "        plt.title('Break of Structure (BOS)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return bos\n",
        "\n",
        "    # دالة لحساب تغير الشخصية (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "\n",
        "        # رسم بياني لمناطق CHOCH\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(data.index, data['Close'], label='Price')\n",
        "        plt.scatter(choch, data['Close'][choch], marker='x', color='orange', label='CHOCH')\n",
        "        plt.title('Change of Character (CHOCH)')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return choch\n",
        "\n",
        "    # دالة لحساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "\n",
        "        # رسم بياني لمناطق العرض والطلب\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(data.index, data['Close'], label='Price')\n",
        "        for zone in supply_zones:\n",
        "            plt.hlines(zone[1], zone[0], zone[0] + 10, colors='red', linestyles='dashed', label='Supply Zone' if zone == supply_zones[0] else \"\")\n",
        "        for zone in demand_zones:\n",
        "            plt.hlines(zone[1], zone[0], zone[0] + 10, colors='green', linestyles='dashed', label='Demand Zone' if zone == demand_zones[0] else \"\")\n",
        "        plt.title('Supply and Demand Zones')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # دالة لحساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "\n",
        "        # رسم بياني لخطوط الدعم والمقاومة\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(data.index, data['Close'], label='Price')\n",
        "        plt.plot(data.index, support, label='Support', color='green')\n",
        "        plt.plot(data.index, resistance, label='Resistance', color='red')\n",
        "        plt.title('Support and Resistance')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        return support, resistance\n",
        "    # ... (باقي الدوال المساعدة كما هي)\n",
        "   # ... (باقي كود بناء الاستراتيجية كما هو)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NXolTKXcwJSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Function to analyze economic news sentiment from Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        # ... (Code for fetching and processing economic calendar data from Investing.com)\n",
        "        # ... (Code to perform sentiment analysis using TextBlob)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # Function to calculate order blocks\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        # ... (Code to calculate order blocks based on price swings)\n",
        "        # ... (Code to create a plot of the order blocks)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # Function to calculate BOS regions\n",
        "    def calculate_bos(data):\n",
        "        # ... (Code to calculate BOS)\n",
        "        # ... (Code to create a plot of BOS)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    # Function to calculate CHOCH\n",
        "    def calculate_choch(data):\n",
        "        # ... (Code to calculate CHOCH)\n",
        "        # ... (Code to create a plot of CHOCH)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    # Function to calculate supply and demand zones\n",
        "    def calculate_supply_demand(data):\n",
        "        # ... (Code to calculate supply and demand zones)\n",
        "        # ... (Code to create a plot of supply and demand zones)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    # Function to calculate support and resistance lines\n",
        "    def calculate_support_resistance(data):\n",
        "        # ... (Code to calculate support and resistance lines)\n",
        "        # ... (Code to create a plot of support and resistance lines)\n",
        "        pass  # You'll need to fill in this part\n",
        "\n",
        "    # --- RSI Calculation and Plotting ---\n",
        "    df['RSI'] = talib.RSI(df['Close'], timeperiod=14)  # Calculate RSI with a 14-day period\n",
        "\n",
        "    # Plot RSI\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df.index, df['RSI'], label='RSI')\n",
        "    plt.title('Relative Strength Index (RSI)')\n",
        "    plt.axhline(70, color='red', linestyle='--', label='Overbought (70)')\n",
        "    plt.axhline(30, color='green', linestyle='--', label='Oversold (30)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zGk5CEOOzVQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar xzf ta-lib-0.4.0-src.tar.gz\n",
        "%cd ta-lib\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "%cd .."
      ],
      "metadata": {
        "id": "HYqECs635e2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ta-lib"
      ],
      "metadata": {
        "id": "Fd-dPBkP5fJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "id": "-YGXd6hJ5fbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    def clean_data(data):\n",
        "        \"\"\"تنظيف البيانات وإزالة القيم المفقودة.\"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    df = clean_data(df)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Function to analyze economic news sentiment from Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\n",
        "                        \"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # Function to calculate Order Blocks\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # Function to calculate Break of Structure (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # Function to calculate Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # Function to calculate Supply and Demand Zones\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # Function to calculate Support and Resistance Levels\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # Function to calculate Fibonacci Retracement Levels\n",
        "    def fibonacci_retracement(high, low):\n",
        "        diff = high - low\n",
        "        return {\n",
        "            'fib_retracement_0.0': low,\n",
        "            'fib_retracement_23.6': low + diff * 0.236,\n",
        "            'fib_retracement_38.2': low + diff * 0.382,\n",
        "            'fib_retracement_50.0': low + diff * 0.5,\n",
        "            'fib_retracement_61.8': low + diff * 0.618,\n",
        "            'fib_retracement_100.0': high\n",
        "        }\n",
        "\n",
        "    # Function to calculate Relative Strength Index (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب RSI\"\"\"\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "\n",
        "    # Function to calculate Moving Average\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "\n",
        "    # Function to calculate Momentum\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "\n",
        "    # Function to analyze trading volume\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "        data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "        return data\n",
        "\n",
        "    # Function to calculate Wolf Wave\n",
        "    def calculate_wolf_wave(data):\n",
        "        \"\"\"حساب موجة الذئب\"\"\"\n",
        "        wave_points = []\n",
        "        for i in range(1, len(data) - 4):\n",
        "            if (data['Close'][i] < data['Close'][i - 1] and\n",
        "                    data['Close'][i + 1] < data['Close'][i] and\n",
        "                    data['Close'][i + 2] > data['Close'][i + 1] and\n",
        "                    data['Close'][i + 3] < data['Close'][i + 2] and\n",
        "                    data['Close'][i + 4] > data['Close'][i + 3]):\n",
        "                wave_points.append(data.index[i])\n",
        "        return wave_points\n",
        "\n",
        "    # Function to create additional features\n",
        "    def create_features(data):\n",
        "        \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "        data['Price_Change'] = data['Close'].pct_change()\n",
        "        data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "        data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "        return data\n",
        "\n",
        "    # Function to calculate Price Channel\n",
        "    def calculate_price_channel(data, window=20):\n",
        "        \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "        data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "        data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "        data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "        return data\n",
        "\n",
        "    # Function for advanced risk management\n",
        "    def advanced_risk_management(data):\n",
        "        \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "        # Calculate ATR (Average True Range)\n",
        "        data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "\n",
        "        # Determine Stop Loss levels\n",
        "        data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # Stop Loss at 1.5 ATR\n",
        "        data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)  # Take Profit at 3 ATR\n",
        "\n",
        "        return data\n",
        "\n",
        "    # Function to build the complete strategy\n",
        "    def build_strategy(data):\n",
        "        \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "        data = clean_data(data)  # Clean data\n",
        "\n",
        "        # Calculate Order Blocks, BOS, CHOCH\n",
        "        data = calculate_order_blocks(data)\n",
        "        bos = calculate_bos(data)\n",
        "        choch = calculate_choch(data)\n",
        "\n",
        "        # Calculate Supply and Demand Zones\n",
        "        supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "        # Calculate Support and Resistance Levels\n",
        "        support, resistance = calculate_support_resistance(data)\n",
        "        data['Support'] = support\n",
        "        data['Resistance'] = resistance\n",
        "\n",
        "        # Calculate Fibonacci Retracement Levels\n",
        "        fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "        for key, value in fib_levels.items():\n",
        "            data[key] = value\n",
        "\n",
        "        # Calculate RSI\n",
        "        data = calculate_rsi(data)\n",
        "\n",
        "        # Calculate Moving Average\n",
        "        data = calculate_moving_average(data)\n",
        "\n",
        "        # Calculate Momentum\n",
        "        data = calculate_momentum(data)\n",
        "\n",
        "        # Analyze Trading Volume\n",
        "        data = analyze_volume(data)\n",
        "\n",
        "        # Calculate Wolf Wave\n",
        "        wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "        # Create Additional Features\n",
        "        data = create_features(data)\n",
        "\n",
        "        # Calculate Price Channel\n",
        "        data = calculate_price_channel(data)\n",
        "\n",
        "        # Advanced Risk Management\n",
        "        data = advanced_risk_management(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "    # Build and apply the strategy\n",
        "    df = build_strategy(df)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# عرض النتائج\n",
        "    print(\"Order Blocks:\", order_blocks)\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    print(\"Wolf Wave Points:\", wave_points)\n",
        "\n",
        "    # تحليل المشاعر\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    data['Sentiment'] = np.mean(sentiments)\n",
        "\n",
        "    # تحديد الميزات والهدف\n",
        "    features = data.drop(['target'], axis=1)\n",
        "    target = data['target']\n",
        "\n",
        "    # تقسيم البيانات\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # نموذج XGBoost\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج Random Forest\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج SVM\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # توليد الإشارات\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "L3MpP18JsKyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta-lib\n",
        "!pip install keras\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "6wII66nqn5_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    def clean_data(data):\n",
        "        \"\"\"تنظيف البيانات وإزالة القيم المفقودة.\"\"\"\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    df = clean_data(df)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Function to analyze economic news sentiment from Investing.com\n",
        "    def get_investing_economic_calendar():\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\n",
        "                        \"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\n",
        "                        \"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # Function to calculate Order Blocks\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        # Iterate using df.index to get row labels\n",
        "        for i in df.index[1:-1]:\n",
        "            if df.loc[i, 'high_min'] > df.loc[i - 1, 'low_max'] and df.loc[i, 'high_min'] > df.loc[i + 1, 'low_max']:\n",
        "                df.loc[i, 'buy_ob'] = df.loc[i, 'high_min']\n",
        "            if df.loc[i, 'low_max'] < df.loc[i - 1, 'high_min'] and df.loc[i, 'low_max'] < df.loc[i + 1, 'high_min']:\n",
        "                df.loc[i, 'sell_ob'] = df.loc[i, 'low_max']\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    # Function to calculate Break of Structure (BOS)\n",
        "    def calculate_bos(data):\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])  # Append the index (date or time)\n",
        "        return bos\n",
        "\n",
        "    # Function to calculate Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])  # Append the index (date or time)\n",
        "        return choch\n",
        "\n",
        "    # Function to calculate Supply and Demand Zones\n",
        "    def calculate_supply_demand(data):\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))  # Append index and price\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))  # Append index and price\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # Function to calculate Support and Resistance Levels\n",
        "    def calculate_support_resistance(data):\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # حساب مستويات فيبوناتش\n",
        "    def fibonacci_retracement(high, low):\n",
        "    \"\"\"حساب مستويات فيبوناتشي\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n",
        "\n",
        "    # حساب مؤشر القوة النسبية (RSI)\n",
        "    def calculate_rsi(data, period=14):\n",
        "    \"\"\"حساب RSI\"\"\"\n",
        "    data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "    return data\n",
        "\n",
        "# حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "    \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "    data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "    return data\n",
        "\n",
        "# حساب الزخم (Momentum)\n",
        "    def calculate_momentum(data, period=10):\n",
        "    \"\"\"حساب الزخم\"\"\"\n",
        "    data['Momentum'] = data['Close'].diff(periods=period)\n",
        "    return data\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "    \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    return data\n",
        "\n",
        "# حساب موجة الذئب\n",
        "    def calculate_wolf_wave(data):\n",
        "    \"\"\"حساب موجة الذئب\"\"\"\n",
        "    wave_points = []\n",
        "    for i in range(1, len(data)-4):\n",
        "        if (data['Close'][i] < data['Close'][i-1] and\n",
        "            data['Close'][i+1] < data['Close'][i] and\n",
        "            data['Close'][i+2] > data['Close'][i+1] and\n",
        "            data['Close'][i+3] < data['Close'][i+2] and\n",
        "            data['Close'][i+4] > data['Close'][i+3]):\n",
        "            wave_points.append(data.index[i])\n",
        "    return wave_points\n",
        "\n",
        "# إنشاء ميزات إضافية\n",
        "    def create_features(data):\n",
        "    \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "    data['Price_Change'] = data['Close'].pct_change()\n",
        "    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "    return data\n",
        "\n",
        "# حساب مؤشر القنوات السعرية\n",
        "    def calculate_price_channel(data, window=20):\n",
        "    \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "    data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "    data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "    data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "    return data\n",
        "\n",
        "# تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "    def advanced_risk_management(data):\n",
        "    \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "    # حساب ATR (Average True Range)\n",
        "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "\n",
        "    # تحديد مستويات وقف الخسارة\n",
        "    data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "    data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "    return data\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "    def build_strategy(data):\n",
        "    \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "    data = clean_data(data)\n",
        "\n",
        "    # حساب Order Blocks وBOS وCHOCH\n",
        "    order_blocks = calculate_order_blocks(data)\n",
        "    bos = calculate_bos(data)\n",
        "    choch = calculate_choch(data)\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    data['Support'] = support\n",
        "    data['Resistance'] = resistance\n",
        "\n",
        "    # حساب مستويات فيبوناتشي\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "\n",
        "    # حساب RSI\n",
        "    data = calculate_rsi(data)\n",
        "\n",
        "    # حساب Moving Average\n",
        "    data = calculate_moving_average(data)\n",
        "\n",
        "    # حساب Momentum\n",
        "    data = calculate_momentum(data)\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    data = analyze_volume(data)\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    data = create_features(data)\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    data = calculate_price_channel(data)\n",
        "\n",
        "    # تحسين إدارة المخاطر\n",
        "    data = advanced_risk_management(data)\n",
        "\n",
        "    # عرض النتائج\n",
        "    print(\"Order Blocks:\", order_blocks)\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    print(\"Wolf Wave Points:\", wave_points)\n",
        "\n",
        "    # تحليل المشاعر\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    data['Sentiment'] = np.mean(sentiments)\n",
        "\n",
        "    # تحديد الميزات والهدف\n",
        "    features = data.drop(['target'], axis=1)\n",
        "    target = data['target']\n",
        "\n",
        "    # تقسيم البيانات\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # نموذج XGBoost\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج Random Forest\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج SVM\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # توليد الإشارات\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n",
        "    return signals_xgb, signals_rf, signals_svr\n",
        "\n",
        "# تحليل النتائج\n",
        "    def analyze_results(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل النتائج ورسم الرسوم البيانية\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(data.index, data['Close'], label='Close Price', color='blue')\n",
        "    plt.plot(data.index[-len(signals_xgb):], signals_xgb, label='XGBoost Signals', color='orange')\n",
        "    plt.plot(data.index[-len(signals_rf):], signals_rf, label='Random Forest Signals', color='green')\n",
        "    plt.plot(data.index[-len(signals_svr):], signals_svr, label='SVM Signals', color='red')\n",
        "    plt.plot(data.index, data['Channel_Upper'], label='Upper Channel', linestyle='--', color='purple')\n",
        "    plt.plot(data.index, data['Channel_Lower'], label='Lower Channel', linestyle='--', color='brown')\n",
        "    plt.title('Gold Price and Model Predictions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price / Signals')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# إدارة المخاطر\n",
        "    def risk_management(data, signals):\n",
        "    \"\"\"إدارة المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "    def execute_trades(data, signals):\n",
        "    \"\"\"تنفيذ التداولات بناءً على الإشارات\"\"\"\n",
        "    for i, signal in enumerate(signals):\n",
        "        if signal > 0:\n",
        "            print(f\"شراء عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "        elif signal < 0:\n",
        "            print(f\"بيع عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "    def periodic_improvement(data):\n",
        "    \"\"\"تحسين النظام بشكل دوري\"\"\"\n",
        "    # تحليل الأداء بشكل دوري\n",
        "    pass\n",
        "\n",
        "# إضافة دالة لتحزين النتائج\n",
        "    def save_results_to_csv(data, filename='results.csv'):\n",
        "    \"\"\"تخزين النتائج في ملف CSV\"\"\"\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "# إضافة تحليل شامل لجميع النماذج\n",
        "    def analyze_all_models(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل شامل لجميع النماذج\"\"\"\n",
        "    print(\"تحليل أداء النماذج:\")\n",
        "    print(f'Total Return XGBoost: {signals_xgb.sum()}')\n",
        "    print(f'Total Return Random Forest: {signals_rf.sum()}')\n",
        "    print(f'Total Return SVM: {signals_svr.sum()}')\n",
        "\n",
        "# إضافة تحليل المخاطر\n",
        "    def risk_analysis(data):\n",
        "    \"\"\"تحليل المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# إضافة تحليل الارتباط\n",
        "    def correlation_analysis(data):\n",
        "    \"\"\"تحليل الارتباط بين الميزات المختلفة\"\"\"\n",
        "    correlation = data.corr()\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحليل زمني\n",
        "def time_series_analysis(data):\n",
        "    \"\"\"تحليل السلاسل الزمنية\"\"\"\n",
        "    seasonal_decomp = seasonal_decompose(data['Close'], model='additive', period=30)\n",
        "    seasonal_decomp.plot()\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحسينات على نماذج التعلم الآلي\n",
        "def train_neural_network(X_train, y_train):\n",
        "    \"\"\"تدريب الشبكة العصبية\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))  # الخرج النهائي\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "    return model\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "    data = load_data('gold_price_data.csv')\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "    signals_xgb, signals_rf, signals_svr = build_strategy(data)\n",
        "\n",
        "# تحليل النتائج\n",
        "    analyze_results(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# إدارة المخاطر\n",
        "    risk_management(data, signals_xgb)\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "    execute_trades(data, signals_xgb)\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "    periodic_improvement(data)\n",
        "\n",
        "# تخزين النتائج\n",
        "    save_results_to_csv(data)\n",
        "\n",
        "# تحليل شامل لجميع النماذج\n",
        "    analyze_all_models(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# تحليل المخاطر\n",
        "    risk_analysis(data)\n",
        "\n",
        "# تحليل الارتباط\n",
        "    correlation_analysis(data)\n",
        "\n",
        "# تحليل زمني\n",
        "    time_series_analysis(data)\n",
        "\n",
        "# تدريب الشبكة العصبية\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data.drop(['target'], axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "    nn_model = train_neural_network(X_train, y_train)\n",
        "\n",
        "# تنفيذ الاستراتيجية باستخدام الشبكة العصبية\n",
        "    def generate_signals(model, data):\n",
        "    \"\"\"توليد الإشارات باستخدام النموذج المدرب\"\"\"\n",
        "    return model.predict(data)\n",
        "\n",
        "    nn_signals = generate_signals(nn_model, X_test)\n",
        "\n",
        "# تقييم أداء الشبكة العصبية\n",
        "    def evaluate_performance(signals, data):\n",
        "    \"\"\"تقييم أداء النموذج\"\"\"\n",
        "    total_return = data['Returns'].sum()\n",
        "    print(f'Total Return: {total_return}')\n",
        "\n",
        "    evaluate_performance(nn_signals, data)\n",
        "\n",
        "# مقارنة النماذج\n",
        "    def compare_models(signals_xgb, nn_signals):\n",
        "    \"\"\"مقارنة أداء النماذج المختلفة\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(data.index, signals_xgb.cumsum(), label='XGBoost Cumulative Returns', color='blue')\n",
        "    plt.plot(data.index, nn_signals.cumsum(), label='Neural Network Cumulative Returns', color='orange')\n",
        "    plt.title('Model Comparison: XGBoost vs Neural Network')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Cumulative Returns')\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    compare_models(signals_xgb, nn_signals)\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "    periodic_improvement(data)\n",
        "\n",
        "# إضافة دالة لتحليل البيانات التاريخية\n",
        "    def historical_analysis(data):\n",
        "    \"\"\"تحليل البيانات التاريخية\"\"\"\n",
        "    historical_returns = data['Close'].pct_change().dropna()\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.hist(historical_returns, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "    plt.title('Historical Returns Distribution')\n",
        "    plt.xlabel('Returns')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "historical_analysis(data)\n",
        "\n",
        "# إظهار النتائج النهائية\n",
        "print(\"الاستراتيجية مكتملة.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ... (rest of your code)\n",
        "\n",
        "    # Call the functions and store the results\n",
        "    bos = calculate_bos(df)\n",
        "    choch = calculate_choch(df)\n",
        "    supply_zones, demand_zones = calculate_supply_demand(df)\n",
        "\n",
        "    # ... (rest of your code)\n",
        "\n",
        "    # عرض النتائج\n",
        "    print(\"DataFrame with Order Blocks:\", df_with_obs)  # Use df_with_obs\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    # ... (rest of your code)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "QgyCvGcGkLu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    # ... (rest of the code for technical indicators, features, and models)\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "\n",
        "        # ... (حساب المؤشرات الفنية كما هو) ...\n",
        "\n",
        "        # دمج بيانات الأخبار مع بيانات السعر (إذا توفرت)\n",
        "        if df_news is not None:\n",
        "            # ... (دمج البيانات بناءً على الوقت) ...\n",
        "            pass  # Replace with your merging logic\n",
        "\n",
        "        # ... (بناء نموذج XGBoost كما هو) ...\n",
        "\n",
        "        # ... (بناء نموذج RandomForest كما هو) ...\n",
        "\n",
        "        # ... (بناء نموذج SVR كما هو) ...\n",
        "\n",
        "        # توليد إشارات التداول\n",
        "        # ... (استخدم النماذج للتنبؤ واتخاذ قرارات التداول) ...\n",
        "        pass  # Replace with your signal generation logic\n",
        "\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-D3Z92IJyalZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # حساب Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # حساب Fibonacci Retracement\n",
        "    def fibonacci_retracement(data,high_col='High', low_col='Low'):\n",
        "        \"\"\"حساب مستويات Fibonacci Retracement\"\"\"\n",
        "        high = data[high_col].max()\n",
        "        low = data[low_col].min()\n",
        "        diff = high - low\n",
        "\n",
        "        levels = {\n",
        "            '0.0': low,\n",
        "            '0.236': low + 0.236 * diff,\n",
        "            '0.382': low + 0.382 * diff,\n",
        "            '0.5': low + 0.5 * diff,\n",
        "            '0.618': low + 0.618 * diff,\n",
        "            '1.0': high\n",
        "        }\n",
        "        return levels #... (كود حساب Fibonacci Retracement) ...\n",
        "        pass  # استبدلت هذا بالكود الفعلي\n",
        "\n",
        "    # حساب RSI\n",
        "    def calculate_rsi(data, period=14):\n",
        "        \"\"\"حساب مؤشر القوة النسبية (RSI)\"\"\"\n",
        "        # ... (كود حساب RSI) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        # ... (كود حساب المتوسط المتحرك) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # حساب الزخم\n",
        "    def calculate_momentum(data, period=10):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        # ... (كود حساب الزخم) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data):\n",
        "        \"\"\"تحليل حجم التداول\"\"\"\n",
        "        # ... (كود تحليل حجم التداول) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # حساب نمط Wolf Wave\n",
        "    def calculate_wolf_wave(data):\n",
        "        \"\"\"حساب نمط Wolf Wave\"\"\"\n",
        "        # ... (كود حساب نمط Wolf Wave) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    def create_features(data):\n",
        "        \"\"\"إنشاء ميزات إضافية مثل تغييرات الأسعار والتقلبات\"\"\"\n",
        "        # ... (كود إنشاء ميزات إضافية) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # حساب قناة السعر\n",
        "    def calculate_price_channel(data):\n",
        "        \"\"\"حساب قناة السعر (العلوي والسفلي والمتوسط)\"\"\"\n",
        "        # ... (كود حساب قناة السعر) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # إدارة المخاطر المتقدمة\n",
        "    def advanced_risk_management(data):\n",
        "        \"\"\"حساب معلمات إدارة المخاطر مثل وقف الخسارة وجني الأرباح باستخدام متوسط المدى الحقيقي (ATR)\"\"\"\n",
        "        # ... (كود إدارة المخاطر المتقدمة) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "\n",
        "        # حساب المؤشرات الفنية\n",
        "        df = calculate_order_blocks(df)\n",
        "        bos_regions = calculate_bos(df)\n",
        "        choch_regions = calculate_choch(df)\n",
        "        supply_zones, demand_zones = calculate_supply_demand(df)\n",
        "        support, resistance = calculate_support_resistance(df)\n",
        "        # ... (حساب باقي المؤشرات الفنية) ...\n",
        "\n",
        "        # دمج بيانات الأخبار مع بيانات السعر (إذا توفرت)\n",
        "        if df_news is not None:\n",
        "            # دمج البيانات بناءً على الوقت -  يتطلب تعديل حسب تنسيق الوقت في كلا الجدولين\n",
        "            # df = pd.merge(df, df_news, left_on='Date', right_on='time', how='left')  # مثال\n",
        "            pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "        # بناء نموذج XGBoost\n",
        "        X = df[['Close', 'Volume', 'RSI', 'MA']]  # استبدل بالميزات الفعلية\n",
        "        y = df['Close'].shift(-1)  # استبدل بالهدف الفعلي\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        xgb_model = XGBRegressor()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # بناء نموذج RandomForest\n",
        "        rf_model = RandomForestRegressor()\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # بناء نموذج SVR\n",
        "        svr_model = SVR()\n",
        "        svr_model.fit(X_train, y_train)\n",
        "\n",
        "        # توليد إشارات التداول\n",
        "        # ... (استخدم النماذج للتنبؤ واتخاذ قرارات التداول) ...\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IlledSDM9HvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#و العباس هذا الكود الاصلي\n",
        "!pip install ta-lib\n",
        "#!pip install keras\n",
        "#!pip install matplotlib\n",
        "#!pip install xgboost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# تركيب جوجل درايف\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# تحديد مسار الملف في جوجل درايف\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # استبدل بمسار الملف الفعلي\n",
        "\n",
        "try:\n",
        "    # قراءة ملف CSV في DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for column in ['Close','Open','High','Low','Volume']:\n",
        "        df[column]=pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "\n",
        "    # تنظيف البيانات (إزالة القيم المفقودة)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"\n",
        "        يقوم بقراءة عناوين المفكرة الاقتصادية من موقع Investing.com وتحليل مشاعرها.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"  # رابط المفكرة الاقتصادية\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # تحليل مشاعر الأخبار\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"حدث خطأ في طلب البيانات: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "            return None  # Return None in case of error\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    if 'df' in locals() and not df.empty:\n",
        "        df_with_obs = calculate_order_blocks(df)\n",
        "        print(\"\\nDataFrame with Order Blocks:\")\n",
        "        print(df_with_obs)\n",
        "    else:\n",
        "        print(\"Error: DataFrame 'df' is not defined or empty.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    # حساب Change of Character (CHOCH)\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "                    (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    def calculate_support_resistance(data):\n",
        "        \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    # حساب Fibonacci Retracement\n",
        "    def fibonacci_retracement(data, high_col='High', low_col='Low', price_col='Close'):\n",
        "        \"\"\"حساب مستويات Fibonacci Retracement\"\"\"\n",
        "        high = data[high_col].max()\n",
        "        low = data[low_col].min()\n",
        "        diff = high - low\n",
        "        levels = [high - (diff * level) for level in [0, 0.236, 0.382, 0.5, 0.618, 0.786, 1]]\n",
        "        return levels\n",
        "\n",
        "    # حساب RSI\n",
        "    def calculate_rsi(data, period=14, price_col='Close'):\n",
        "        \"\"\"حساب مؤشر القوة النسبية (RSI)\"\"\"\n",
        "        prices = data[price_col].values\n",
        "        rsi = talib.RSI(prices, timeperiod=period)\n",
        "        return rsi\n",
        "\n",
        "    # حساب المتوسط المتحرك\n",
        "    def calculate_moving_average(data, period=20, price_col='Close'):\n",
        "        \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "        prices = data[price_col].values\n",
        "        ma = talib.SMA(prices, timeperiod=period)\n",
        "        return ma\n",
        "\n",
        "    # حساب الزخم\n",
        "    def calculate_momentum(data, period=10, price_col='Close'):\n",
        "        \"\"\"حساب الزخم\"\"\"\n",
        "        prices = data[price_col].values\n",
        "        momentum = talib.MOM(prices, timeperiod=period)\n",
        "        return momentum\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    def analyze_volume(data, volume_col='Volume'):\n",
        "        \"\"\"تحليل حجم التداول\"\"\"\n",
        "        # ... (كود تحليل حجم التداول) ...\n",
        "        # This is just a placeholder. Add more advanced volume analysis here\n",
        "        avg_volume = data[volume_col].mean()\n",
        "        return avg_volume\n",
        "\n",
        "    # حساب نمط Wolf Wave\n",
        "    def calculate_wolf_wave(data, high_col='High', low_col='Low'):\n",
        "        \"\"\"حساب نمط Wolf Wave\"\"\"\n",
        "        # ... (كود حساب نمط Wolf Wave) ...\n",
        "        # This is a complex pattern. Consider using a dedicated library or custom logic.\n",
        "        # This placeholder simply checks for basic price swings.\n",
        "        wolf_wave = []\n",
        "        for i in range(4, len(data)):\n",
        "         if data[high_col][i] > data[high_col][i - 2] and \\\n",
        "                    data[low_col][i] < data[low_col][i - 2] and \\\n",
        "                    data[high_col][i - 2] < data[high_col][i - 4] and \\\n",
        "                    data[low_col][i - 2] > data[low_col][i - 4]:\n",
        "                wolf_wave.append(data.index[i])\n",
        "        return wolf_wave\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    def create_features(data, price_col='Close'):\n",
        "        \"\"\"إنشاء ميزات إضافية مثل تغييرات الأسعار والتقلبات\"\"\"\n",
        "        data['price_change'] = data[price_col].diff()\n",
        "        data['volatility'] = data[price_col].rolling(window=10).std()\n",
        "        return data\n",
        "\n",
        "    # حساب قناة السعر\n",
        "    def calculate_price_channel(data, period=20, price_col='Close'):\n",
        "        \"\"\"حساب قناة السعر (العلوي والسفلي والمتوسط)\"\"\"\n",
        "        data['upper_channel'] = data[price_col].rolling(window=period).max()\n",
        "        data['lower_channel'] = data[price_col].rolling(window=period).min()\n",
        "        data['mid_channel'] = (data['upper_channel'] + data['lower_channel']) / 2\n",
        "        return data\n",
        "\n",
        "    # إدارة المخاطر المتقدمة\n",
        "    def advanced_risk_management(data, price_col='Close', risk_factor=0.02):\n",
        "        \"\"\"حساب معلمات إدارة المخاطر مثل وقف الخسارة وجني الأرباح باستخدام متوسط المدى الحقيقي (ATR)\"\"\"\n",
        "        data['ATR'] = talib.ATR(data['High'].values, data['Low'].values, data[price_col].values, timeperiod=14)\n",
        "        data['stop_loss'] = data[price_col] - (data['ATR'] * risk_factor)\n",
        "        data['take_profit'] = data[price_col] + (data['ATR'] * risk_factor)\n",
        "        return data\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "\n",
        "        # حساب المؤشرات الفنية\n",
        "        df = calculate_order_blocks(df)\n",
        "        bos_regions = calculate_bos(df)\n",
        "        choch_regions = calculate_choch(df)\n",
        "        supply_zones, demand_zones = calculate_supply_demand(df)\n",
        "        support, resistance = calculate_support_resistance(df)\n",
        "        fib_levels = fibonacci_retracement(df)\n",
        "        df['RSI'] = calculate_rsi(df)\n",
        "        df['MA'] = calculate_moving_average(df)\n",
        "        df['Momentum'] = calculate_momentum(df)\n",
        "        avg_volume = analyze_volume(df)\n",
        "        wolf_wave_regions = calculate_wolf_wave(df)\n",
        "        df = create_features(df)\n",
        "        df = calculate_price_channel(df)\n",
        "        df = advanced_risk_management(df)\n",
        "\n",
        "        df['SMA_20'] = talib.SMA(df['Close'], timeperiod=20)\n",
        "        df['EMA_20'] = talib.EMA(df['Close'], timeperiod=20)\n",
        "        df['RSI'] = talib.RSI(df['Close'], timeperiod=14)\n",
        "        df['Stochastic_K'], df['Stochastic_D'] = talib.STOCH(df['High'], df['Low'], df['Close'])\n",
        "        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = talib.MACD(df['Close'])\n",
        "        df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
        "        df['Upper_Band'], df['Middle_Band'], df['Lower_Band'] = talib.BBANDS(df['Close'])\n",
        "\n",
        "\n",
        "        # ... (و العباس كلهه كملتهه. حساب باقي المؤشرات الفنية) ...\n",
        "\n",
        "        # دمج بيانات الأخبار مع بيانات السعر (إذا توفرت)\n",
        "\n",
        "\n",
        "        #df['Date'] = pd.to_datetime(df['Date'])  # Convert to datetime if needed\n",
        "        #df_news['time'] = pd.to_datetime(df_news['time'])  # Convert to datetime if needed\n",
        "        #df = pd.merge(df, df_news, left_on='Date', right_on='time', how='left')\n",
        "\n",
        "\n",
        "\n",
        "        if df_news is not None:\n",
        "            # دمج البيانات بناءً على الوقت -  يتطلب تعديل حسب تنسيق الوقت في كلا الجدولين\n",
        "           #df = pd.merge(df, df_news, left_on='Date', right_on='time', how='left')  # مثال\n",
        "           #Assuming 'Date' i\n",
        "           #df and 'time' in df_news are in the same format\n",
        "           #df['Date'] = pd.to_datetime(df['Date'])  # Convert to datetime if needed\n",
        "           #df_news['time'] = pd.to_datetime(df_news['time'])  # Convert to datetime if needed\n",
        "           #df = pd.merge(df, df_news, left_on='Date', right_on='time', how='left')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            pass  # استبدل هذا بالكود الفعلي بس وخرالهاشتاكات\n",
        "\n",
        "\n",
        "            print(df.shape)\n",
        "            print(df.dtypes)\n",
        "            print(df_news.shape)\n",
        "            print(df_news.dtypes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # بناء نموذج XGBoost\n",
        "         #Assuming you have features like 'Close', 'Volume', 'RSI', 'MA', etc.\n",
        "        features = ['Close', 'Volume', 'RSI', 'MA', 'Momentum', 'price_change', 'volatility',\n",
        "                    'upper_channel', 'lower_channel', 'mid_channel', 'ATR', 'stop_loss', 'take_profit']\n",
        "\n",
        "        # Check if all features are present in the DataFrame\n",
        "        missing_features = [f for f in features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Error: Missing features in DataFrame: {missing_features}\")\n",
        "            return\n",
        "\n",
        "        X = df[features]\n",
        "        y = df['Close'].shift(-1)  # Predicting the next day's closing price\n",
        "        X = X.iloc[:-1]  # Remove last row as it has NaN for target\n",
        "        y = y.iloc[:-1]  # Remove last row as it has NaN for target\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        xgb_model = XGBRegressor()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # بناء نموذج RandomForest\n",
        "        rf_model = RandomForestRegressor()\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # بناء نموذج SVR\n",
        "        svr_model = SVR()\n",
        "        svr_model.fit(X_train, y_train)\n",
        "\n",
        "        # توليد إشارات التداول\n",
        "        # Example: Buy if XGBoost predicts price increase, sell if decrease\n",
        "        df['xgb_prediction'] = xgb_model.predict(X)\n",
        "        df['signal'] = 0  # Initialize signal column\n",
        "        df['signal'][df['xgb_prediction'] > df['Close']] = 1  # Buy signal\n",
        "        df['signal'][df['xgb_prediction'] < df['Close']] = -1  # Sell signal\n",
        "\n",
        "        # ... (استخدم النماذج الأخرى للتنبؤ وتحسين إشارات التداول) ...\n",
        "       #منا لحد bass ما ادري شخربطت بس اكدر الغي\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pass  # استبدل هذا بالكود الفعلي\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BBY-3JX49M_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#و العباس هذا الكود الاصلي\n",
        "!pip install ta-lib\n",
        "#!pip install keras\n",
        "#!pip install matplotlib\n",
        "#!pip install xgboost\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "# ... (باقي عمليات الاستيراد) ...\n",
        "\n",
        "\n",
        "def build_strategy(df, df_news=None):\n",
        "    \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "    # ... (حساب المؤشرات الفنية) ...\n",
        "\n",
        "    # بناء نموذج XGBoost\n",
        "    # ... (بناء النموذج) ...\n",
        "\n",
        "    # توليد إشارات التداول\n",
        "    df['xgb_prediction'] = xgb_model.predict(X)\n",
        "    df['signal'] = 0  # تهيئة عمود الإشارة\n",
        "    df['signal'][df['xgb_prediction'] > df['Close']] = 1  # إشارة شراء\n",
        "    df['signal'][df['xgb_prediction'] < df['Close']] = -1  # إشارة بيع\n",
        "\n",
        "    # طباعة أوامر التداول\n",
        "    print(\"\\nأوامر التداول:\")\n",
        "    for i in range(1, len(df)):\n",
        "        if df['signal'][i] == 1 and df['signal'][i - 1] != 1:  # شراء جديد\n",
        "            print(f\"شراء عند سعر: {df['Close'][i]}, وقف الخسارة: {df['stop_loss'][i]}, جني الأرباح: {df['take_profit'][i]}\")\n",
        "        elif df['signal'][i] == -1 and df['signal'][i - 1] != -1:  # بيع جديد\n",
        "            print(f\"بيع عند سعر: {df['Close'][i]}, وقف الخسارة: {df['stop_loss'][i]}, جني الأرباح: {df['take_profit'][i]}\")\n",
        "\n",
        "    # ... (باقي الكود) ...\n",
        "\n",
        "# ... (باقي الكود) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dztdGWun93iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_strategy(df):\n",
        "    \"\"\"\n",
        "    يرسم جميع المؤشرات وإشارات التداول على الرسم البياني.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))  # إنشاء الرسم البياني\n",
        "\n",
        "    # رسم سعر الإغلاق\n",
        "    ax.plot(df.index, df['Close'], label='سعر الإغلاق', color='blue')\n",
        "\n",
        "    # رسم خطوط الدعم والمقاومة\n",
        "    ax.plot(df.index, df['support'], label='الدعم', color='green', linestyle='--')\n",
        "    ax.plot(df.index, df['resistance'], label='المقاومة', color='red', linestyle='--')\n",
        "\n",
        "    # رسم مناطق العرض والطلب\n",
        "    for zone in df['supply_zones']:\n",
        "        ax.axhspan(zone[1], zone[1] + 10, alpha=0.2, color='red')  # عرض\n",
        "    for zone in df['demand_zones']:\n",
        "        ax.axhspan(zone[1], zone[1] - 10, alpha=0.2, color='green')  # طلب\n",
        "\n",
        "    # رسم إشارات التداول\n",
        "    buy_signals = df[df['signal'] == 1]\n",
        "    sell_signals = df[df['signal'] == -1]\n",
        "    ax.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='شراء', s=100)\n",
        "    ax.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='بيع', s=100)\n",
        "\n",
        "    # ... (رسم باقي المؤشرات: RSI, MA, Momentum, ...) ...\n",
        "    # مثال: رسم المتوسط المتحرك\n",
        "    ax.plot(df.index, df['MA'], label='المتوسط المتحرك', color='orange')\n",
        "\n",
        "    # ... (رسم باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "\n",
        "    # تخصيص الرسم البياني\n",
        "    ax.set_title('إستراتيجية التداول')\n",
        "    ax.set_xlabel('التاريخ')\n",
        "    ax.set_ylabel('السعر')\n",
        "    ax.legend()  # عرض مفتاح الرسم\n",
        "    plt.grid(True)  # عرض شبكة\n",
        "    plt.show()  # عرض الرسم البياني"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "in5UAmIOvFIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "    for column in ['Close', 'Open', 'High', 'Low', 'Volume']:# Replace with your column names\n",
        "        df[column] = pd.to_numeric(df[column],errors='coerce')\n",
        "\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"Retrieves economic calendar data from Investing.com and performs sentiment analysis.\"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # Sentiment analysis\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    def calculate_support_resistance(data):\n",
        "        \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "        # حساب المؤشرات الفنية\n",
        "        df['RSI'] = talib.RSI(df['Close'], period=14)\n",
        "        df['MA'] = talib.SMA(df['Close'], period=20)\n",
        "        df['Momentum'] = talib.MOM(df['Close'], period=10)\n",
        "        # ... (حساب باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        # حساب مناطق BOS و CHOCH\n",
        "        df['BOS'] = calculate_bos(df)\n",
        "        df['CHOCH'] = calculate_choch(df)\n",
        "\n",
        "        # حساب مناطق العرض والطلب\n",
        "        df['supply_zones'], df['demand_zones'] = calculate_supply_demand(df)\n",
        "\n",
        "        # حساب خطوط الدعم والمقاومة\n",
        "        df['support'], df['resistance'] = calculate_support_resistance(df)\n",
        "\n",
        "        # بناء نموذج XGBoost (مثال)\n",
        "        X = df[['RSI', 'MA', 'Momentum']]  # تحديد المتغيرات المستقلة\n",
        "        y = df['Close']  # تحديد المتغير التابع\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        xgb_model = XGBRegressor()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # ... (بناء نماذج أخرى بنفس الطريقة) ...\n",
        "\n",
        "        # توليد إشارات التداول (مثال بسيط)\n",
        "        df['signal'] = 0.0\n",
        "        df['signal'][df['Close'] > df['MA']] = 1.0  # شراء إذا كان السعر أعلى من المتوسط المتحرك\n",
        "        df['signal'][df['Close'] < df['MA']] = -1.0  # بيع إذا كان السعر أقل من المتوسط المتحرك\n",
        "\n",
        "        # ... (دمج إشارات من نماذج أخرى  المنوتحسينطق) ...\n",
        "\n",
        "        # رسم جميع المؤشرات وإشارات التداول على الرسم البياني\n",
        "        fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "        ax.plot(df.index, df['Close'], label='سعر الإغلاق', color='blue')\n",
        "        ax.plot(df.index, df['support'], label='الدعم', color='green', linestyle='--')\n",
        "        ax.plot(df.index, df['resistance'], label='المقاومة', color='red', linestyle='--')\n",
        "\n",
        "        for zone in df['supply_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] + 10, alpha=0.2, color='red')  # عرض\n",
        "        for zone in df['demand_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] - 10, alpha=0.2, color='green')  # طلب\n",
        "\n",
        "        buy_signals = df[df['signal'] == 1]\n",
        "        sell_signals = df[df['signal'] == -1]\n",
        "        ax.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='شراء', s=100)\n",
        "        ax.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='بيع', s=100)\n",
        "\n",
        "        ax.plot(df.index, df['RSI'], label='RSI', color='purple')\n",
        "        ax.plot(df.index, df['MA'], label='المتوسط المتحرك', color='orange')\n",
        "        ax.plot(df.index, df['Momentum'], label='الزخم', color='gray')\n",
        "        # ... (رسم باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        ax.set_title('إستراتيجية التداول')\n",
        "        ax.set_xlabel('التاريخ')\n",
        "        ax.set_ylabel('السعر')\n",
        "        ax.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "GkspQcJZwWA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta-lib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "    # ... (كود قراءة البيانات من ملف CSV كما هو) ...\n",
        "    # تحويل القيم غير الرقمية إلى NaN في أعمدة محددة\n",
        "    for column in ['Close', 'Open', 'High', 'Low']:  # استبدل بأسماء الأعمدة التي تريد تحويلها\n",
        "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"Retrieves economic calendar data from Investing.com and performs sentiment analysis.\"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # Sentiment analysis\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    def calculate_support_resistance(data):\n",
        "        \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "        # حساب المؤشرات الفنية\n",
        "        df['RSI'] = talib.RSI(df['Close'], period=14)\n",
        "        df['MA'] = talib.SMA(df['Close'], period=20)\n",
        "        df['Momentum'] = talib.MOM(df['Close'], period=10)\n",
        "        # ... (حساب باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        # حساب مناطق BOS و CHOCH\n",
        "        df['BOS'] = calculate_bos(df)\n",
        "        df['CHOCH'] = calculate_choch(df)\n",
        "\n",
        "        # حساب مناطق العرض والطلب\n",
        "        df['supply_zones'], df['demand_zones'] = calculate_supply_demand(df)\n",
        "\n",
        "        # حساب خطوط الدعم والمقاومة\n",
        "        df['support'], df['resistance'] = calculate_support_resistance(df)\n",
        "\n",
        "        # بناء نموذج XGBoost (مثال)\n",
        "        X = df[['RSI', 'MA', 'Momentum']]  # تحديد المتغيرات المستقلة\n",
        "        y = df['Close']  # تحديد المتغير التابع\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        xgb_model = XGBRegressor()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # ... (بناء نماذج أخرى بنفس الطريقة) ...\n",
        "\n",
        "        # توليد إشارات التداول (مثال بسيط)\n",
        "        df['signal'] = 0.0\n",
        "        df['signal'][df['Close'] > df['MA']] = 1.0  # شراء إذا كان السعر أعلى من المتوسط المتحرك\n",
        "        df['signal'][df['Close'] < df['MA']] = -1.0  # بيع إذا كان السعر أقل من المتوسط المتحرك\n",
        "\n",
        "        # ... (دمج إشارات من نماذج أخرى وتحسين المنطق) ...\n",
        "\n",
        "        # رسم جميع المؤشرات وإشارات التداول على الرسم البياني\n",
        "        fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "        ax.plot(df.index, df['Close'], label='سعر الإغلاق', color='blue')\n",
        "        ax.plot(df.index, df['support'], label='الدعم', color='green', linestyle='--')\n",
        "        ax.plot(df.index, df['resistance'], label='المقاومة', color='red', linestyle='--')\n",
        "\n",
        "        for zone in df['supply_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] + 10, alpha=0.2, color='red')  # عرض\n",
        "        for zone in df['demand_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] - 10, alpha=0.2, color='green')  # طلب\n",
        "\n",
        "        buy_signals = df[df['signal'] == 1]\n",
        "        sell_signals = df[df['signal'] == -1]\n",
        "        ax.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='شراء', s=100)\n",
        "        ax.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='بيع', s=100)\n",
        "\n",
        "        ax.plot(df.index, df['RSI'], label='RSI', color='purple')\n",
        "        ax.plot(df.index, df['MA'], label='المتوسط المتحرك', color='orange')\n",
        "        ax.plot(df.index, df['Momentum'], label='الزخم', color='gray')\n",
        "        # ... (رسم باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        ax.set_title('إستراتيجية التداول')\n",
        "        ax.set_xlabel('التاريخ')\n",
        "        ax.set_ylabel('السعر')\n",
        "        ax.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "id": "-fHW7FPhwmYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9w-vMbuVl0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3c0dnwg4VsZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install TA-Lib #installing talib library\n",
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib #importing the library after installation\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# File path in Google Drive\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'\n",
        "\n",
        "try:\n",
        "    # Read CSV into DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(df.head())\n",
        "\n",
        "\n",
        "\n",
        "    # ... (كود قراءة البيانات من ملف CSV كما هو) ...\n",
        "    # تحويل القيم غير الرقمية إلى NaN في أعمدة محددة\n",
        "    for column in ['Close', 'Open', 'High', 'Low']:  # استبدل بأسماء الأعمدة التي تريد تحويلها\n",
        "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Clean data (remove missing values)\n",
        "    df.dropna(inplace=True)\n",
        "    print(\"\\nDataFrame after cleaning:\")\n",
        "    print(df.head())\n",
        "\n",
        "    def get_investing_economic_calendar():\n",
        "        \"\"\"Retrieves economic calendar data from Investing.com and performs sentiment analysis.\"\"\"\n",
        "        try:\n",
        "            url = \"https://www.investing.com/economic-calendar/\"\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            news_elements = soup.select(\"table.genTbl.ecoCalTbl > tbody > tr\")\n",
        "            news_data = []\n",
        "\n",
        "            for news_element in news_elements:\n",
        "                title_element = news_element.select_one(\"td.left.event\")\n",
        "                if title_element:\n",
        "                    title = title_element.text.strip()\n",
        "                    time = news_element.select_one(\"td.first.left.time\").text.strip() if news_element.select_one(\"td.first.left.time\") else \"N/A\"\n",
        "                    impact = news_element.select_one(\"td.left.impact\").text.strip() if news_element.select_one(\"td.left.impact\") else \"N/A\"\n",
        "                    previous = news_element.select_one(\"td.left.previous\").text.strip() if news_element.select_one(\"td.left.previous\") else \"N/A\"\n",
        "                    current = news_element.select_one(\"td.left.current\").text.strip() if news_element.select_one(\"td.left.current\") else \"N/A\"\n",
        "                    news_data.append({\n",
        "                        \"title\": title,\n",
        "                        \"time\": time,\n",
        "                        \"impact\": impact,\n",
        "                        \"previous\": previous,\n",
        "                        \"current\": current\n",
        "                    })\n",
        "\n",
        "            df_news = pd.DataFrame(news_data)\n",
        "            print(\"\\nEconomic Calendar Data:\")\n",
        "            print(df_news)\n",
        "\n",
        "            # Sentiment analysis\n",
        "            df_news['sentiment'] = df_news['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
        "            print(\"\\nEconomic Calendar Data with Sentiment Analysis:\")\n",
        "            print(df_news)\n",
        "            return df_news\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    df_news = get_investing_economic_calendar()\n",
        "\n",
        "    def calculate_order_blocks(df, price_column='Close', lookback=10):\n",
        "        \"\"\"Calculates order blocks.\"\"\"\n",
        "        df[price_column] = pd.to_numeric(df[price_column], errors='coerce')\n",
        "        df.dropna(subset=[price_column], inplace=True)\n",
        "        df['high_min'] = df[price_column].rolling(window=lookback, center=True).min()\n",
        "        df['low_max'] = df[price_column].rolling(window=lookback, center=True).max()\n",
        "        df['buy_ob'] = 0\n",
        "        df['sell_ob'] = 0\n",
        "\n",
        "        df_with_order_blocks = calculate_order_blocks(df)\n",
        "        columns_to_display = ['Date', 'Close', 'buy_ob', 'sell_ob']\n",
        "        print(tabulate(df_with_order_blocks[columns_to_display], headers='keys', tablefmt='psql'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(1, len(df) - 1):\n",
        "            if df['high_min'][i] > df['low_max'][i - 1] and df['high_min'][i] > df['low_max'][i + 1]:\n",
        "                df['buy_ob'][i] = df['high_min'][i]\n",
        "            if df['low_max'][i] < df['high_min'][i - 1] and df['low_max'][i] < df['high_min'][i + 1]:\n",
        "                df['sell_ob'][i] = df['low_max'][i]\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def calculate_bos(data):\n",
        "        \"\"\"Calculate BOS regions.\"\"\"\n",
        "        bos = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]:\n",
        "                bos.append(data.index[i])\n",
        "        return bos\n",
        "\n",
        "    def calculate_choch(data):\n",
        "        \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "        choch = []\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i - 1] and data['Close'][i - 1] < data['Close'][i - 2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i - 1] and data['Close'][i - 1] > data['Close'][i - 2]):\n",
        "                choch.append(data.index[i])\n",
        "        return choch\n",
        "\n",
        "    def calculate_supply_demand(data):\n",
        "        \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "        supply_zones = []\n",
        "        demand_zones = []\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i - 1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i - 1]))\n",
        "            elif data['Close'][i] < data['Low'][i - 1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i - 1]))\n",
        "        return supply_zones, demand_zones\n",
        "\n",
        "    def calculate_support_resistance(data):\n",
        "        \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "        support = data['Low'].rolling(window=20).min()\n",
        "        resistance = data['High'].rolling(window=20).max()\n",
        "        return support, resistance\n",
        "\n",
        "    def build_strategy(df, df_news=None):\n",
        "        \"\"\"بناء إستراتيجية التداول.\"\"\"\n",
        "        # حساب المؤشرات الفنية\n",
        "        df['RSI'] = talib.RSI(df['Close'], period=14)\n",
        "        df['MA'] = talib.SMA(df['Close'], period=20)\n",
        "        df['Momentum'] = talib.MOM(df['Close'], period=10)\n",
        "        # ... (حساب باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        # حساب مناطق BOS و CHOCH\n",
        "        df['BOS'] = calculate_bos(df)\n",
        "        df['CHOCH'] = calculate_choch(df)\n",
        "\n",
        "        # حساب مناطق العرض والطلب\n",
        "        df['supply_zones'], df['demand_zones'] = calculate_supply_demand(df)\n",
        "\n",
        "        # حساب خطوط الدعم والمقاومة\n",
        "        df['support'], df['resistance'] = calculate_support_resistance(df)\n",
        "\n",
        "        # بناء نموذج XGBoost (مثال)\n",
        "        X = df[['RSI', 'MA', 'Momentum']]  # تحديد المتغيرات المستقلة\n",
        "        y = df['Close']  # تحديد المتغير التابع\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        xgb_model = XGBRegressor()\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        # ... (بناء نماذج أخرى بنفس الطريقة) ...\n",
        "\n",
        "        # توليد إشارات التداول (مثال بسيط)\n",
        "        df['signal'] = 0.0\n",
        "        df['signal'][df['Close'] > df['MA']] = 1.0  # شراء إذا كان السعر أعلى من المتوسط المتحرك\n",
        "        df['signal'][df['Close'] < df['MA']] = -1.0  # بيع إذا كان السعر أقل من المتوسط المتحرك\n",
        "\n",
        "        # ... (دمج إشارات من نماذج أخرى وتحسين المنطق) ...\n",
        "\n",
        "        # رسم جميع المؤشرات وإشارات التداول على الرسم البياني\n",
        "        fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "        ax.plot(df.index, df['Close'], label='سعر الإغلاق', color='blue')\n",
        "        ax.plot(df.index, df['support'], label='الدعم', color='green', linestyle='--')\n",
        "        ax.plot(df.index, df['resistance'], label='المقاومة', color='red', linestyle='--')\n",
        "\n",
        "        for zone in df['supply_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] + 10, alpha=0.2, color='red')  # عرض\n",
        "        for zone in df['demand_zones']:\n",
        "            ax.axhspan(zone[1], zone[1] - 10, alpha=0.2, color='green')  # طلب\n",
        "\n",
        "        buy_signals = df[df['signal'] == 1]\n",
        "        sell_signals = df[df['signal'] == -1]\n",
        "        ax.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='green', label='شراء', s=100)\n",
        "        ax.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='red', label='بيع', s=100)\n",
        "\n",
        "        ax.plot(df.index, df['RSI'], label='RSI', color='purple')\n",
        "        ax.plot(df.index, df['MA'], label='المتوسط المتحرك', color='orange')\n",
        "        ax.plot(df.index, df['Momentum'], label='الزخم', color='gray')\n",
        "        # ... (رسم باقي المؤشرات بنفس الطريقة) ...\n",
        "\n",
        "        ax.set_title('إستراتيجية التداول')\n",
        "        ax.set_xlabel('التاريخ')\n",
        "        ax.set_ylabel('السعر')\n",
        "        ax.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    # استدعاء دالة بناء الإستراتيجية\n",
        "    build_strategy(df, df_news)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Could not parse CSV file at {file_path}. Check file format.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "MhCqi-fNkkWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip\n"
      ],
      "metadata": {
        "id": "dD_z7usV55po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade setuptools wheel\n"
      ],
      "metadata": {
        "id": "oIVH5XHN6hb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy scipy\n"
      ],
      "metadata": {
        "id": "RB1m9BrA6srs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv my_env"
      ],
      "metadata": {
        "id": "Ctks_nJM7NfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source my_env/bin/activate"
      ],
      "metadata": {
        "id": "FtcEM6yM7cM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install python3.10-venv\n"
      ],
      "metadata": {
        "id": "H7J3z3dh8-GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv my_env\n"
      ],
      "metadata": {
        "id": "qEz6OnzH9FpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source my_env/bin/activate\n"
      ],
      "metadata": {
        "id": "bvh5FlnE9GoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source my_env/bin/activate\n"
      ],
      "metadata": {
        "id": "jqp9lUD2_7Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-ta\n"
      ],
      "metadata": {
        "id": "zeqNcdov9Gr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M29R-pjk9Gxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sVdamjhD9Gz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import talib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the file path for the gold price data\n",
        "file_path = '/content/drive/MyDrive/gold_usd_prices.csv'  # Adjust if your path is different\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"\n",
        "    تحميل البيانات من ملف CSV، وتعيين 'Price' كمؤشر، وتحليل التواريخ.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # قراءة ملف CSV، مع تعيين 'Price' كمؤشر ومحاولة تحليل التواريخ\n",
        "        data = pd.read_csv(filename, index_col='Price', parse_dates=True)\n",
        "\n",
        "        # تحويل أعمدة الأسعار ('Close', 'High', 'Low', 'Volume') إلى قيم رقمية\n",
        "        # باستخدام 'to_numeric' مع 'errors='coerce'' للتعامل مع القيم غير الصالحة\n",
        "        for col in ['Close', 'High', 'Low', 'Volume']:\n",
        "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "\n",
        "        print(\"تم تحميل البيانات بنجاح.\")\n",
        "        return data\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"خطأ: الملف غير موجود في المسار: {filename}\")\n",
        "        return None  # أو يمكنك إرجاع قيمة افتراضية أخرى\n",
        "\n",
        "    except pd.errors.ParserError:\n",
        "        print(f\"خطأ: تعذر تحليل ملف CSV. تحقق من تنسيق الملف: {filename}\")\n",
        "        return None\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"خطأ: عمود 'Price' غير موجود في ملف CSV.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"حدث خطأ غير متوقع: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Clean Data\n",
        "def clean_data(data):\n",
        "    \"\"\"Removes rows with any missing values.\"\"\"\n",
        "    if data is not None:\n",
        "        data.dropna(inplace=True)\n",
        "        print(\"Data cleaned successfully.\")\n",
        "        return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "# Fetch News\n",
        "def fetch_news():\n",
        "    \"\"\"Fetches news titles from Investing.com.\"\"\"\n",
        "    url = \"https://www.investing.com/news/commodities-news/gold\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        news_headers = soup.select('div.news-item-header')\n",
        "        news_titles = [header.text.strip() for header in news_headers]\n",
        "        print(\"News fetched successfully.\")\n",
        "        return news_titles\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching news: {e}\")\n",
        "        return []\n",
        "\n",
        "# Analyze Sentiment\n",
        "def analyze_sentiment(news_titles):\n",
        "    \"\"\"Analyzes sentiment of news titles using TextBlob.\"\"\"\n",
        "    sentiments = []\n",
        "    for title in news_titles:\n",
        "        analysis = TextBlob(title)\n",
        "        score = analysis.sentiment.polarity\n",
        "        sentiments.append(score)\n",
        "    print(\"Sentiment analyzed successfully.\")\n",
        "    return sentiments\n",
        "\n",
        "# Calculate Order Blocks\n",
        "def calculate_order_blocks(data):\n",
        "    \"\"\"Identifies order block zones.\"\"\"\n",
        "    order_blocks = []\n",
        "    if data is not None:\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]:\n",
        "                order_blocks.append((data.index[i-1], data['Close'][i-1]))\n",
        "    return order_blocks\n",
        "\n",
        "# Calculate Break of Structure (BOS)\n",
        "def calculate_bos(data):\n",
        "    \"\"\"Identifies break of structure points.\"\"\"\n",
        "    bos = []\n",
        "    if data is not None:\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "                bos.append(data.index[i])\n",
        "    return bos\n",
        "\n",
        "# Calculate Change of Character (CHOCH)\n",
        "def calculate_choch(data):\n",
        "    \"\"\"Identifies change of character points.\"\"\"\n",
        "    choch = []\n",
        "    if data is not None:\n",
        "        for i in range(1, len(data)):\n",
        "            if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "               (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "                choch.append(data.index[i])\n",
        "    return choch\n",
        "\n",
        "# Calculate Supply and Demand Zones\n",
        "def calculate_supply_demand(data):\n",
        "    \"\"\"Identifies supply and demand zones.\"\"\"\n",
        "    supply_zones = []\n",
        "    demand_zones = []\n",
        "    if data is not None:\n",
        "        for i in range(1, len(data)):\n",
        "            if data['Close'][i] > data['High'][i-1]:\n",
        "                supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "            elif data['Close'][i] < data['Low'][i-1]:\n",
        "                demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "    return supply_zones, demand_zones\n",
        "\n",
        "# Calculate Support and Resistance Lines\n",
        "def calculate_support_resistance(data):\n",
        "    \"\"\"Calculates support and resistance levels using rolling min/max.\"\"\"\n",
        "    if data is not None:\n",
        "      support = data['Low'].rolling(window=20).min()\n",
        "      resistance = data['High'].rolling(window=20).max()\n",
        "      return support, resistance\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Calculate Fibonacci Retracement Levels\n",
        "def fibonacci_retracement(high, low):\n",
        "    \"\"\"Calculates Fibonacci retracement levels.\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n",
        "\n",
        "# Calculate Relative Strength Index (RSI)\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"Calculates the RSI.\"\"\"\n",
        "    if data is not None:\n",
        "        data['RSI'] = talib.RSI(data['Close'], timeperiod=period)\n",
        "        return data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate Moving Average\n",
        "def calculate_moving_average(data, period=20):\n",
        "    \"\"\"Calculates the moving average.\"\"\"\n",
        "    if data is not None:\n",
        "        data['MA'] = data['Close'].rolling(window=period).mean()\n",
        "        return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "\n",
        "# Calculate Momentum\n",
        "def calculate_momentum(data, period=10):\n",
        "    \"\"\"Calculates the momentum.\"\"\"\n",
        "    if data is not None:\n",
        "        data['Momentum'] = data['Close'].diff(periods=period)\n",
        "        return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "# Analyze Volume\n",
        "def analyze_volume(data):\n",
        "    \"\"\"Calculates moving average of volume.\"\"\"\n",
        "    if data is not None:\n",
        "      data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "      return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "\n",
        "# Calculate Wolf Wave\n",
        "def calculate_wolf_wave(data):\n",
        "    \"\"\"Identifies Wolf Wave points.\"\"\"\n",
        "    wave_points = []\n",
        "    if data is not None:\n",
        "        for i in range(1, len(data)-4):\n",
        "            if (data['Close'][i] < data['Close'][i-1] and\n",
        "                data['Close'][i+1] < data['Close'][i] and\n",
        "                data['Close'][i+2] > data['Close'][i+1] and\n",
        "                data['Close'][i+3] < data['Close'][i+2] and\n",
        "                data['Close'][i+4] > data['Close'][i+3]):\n",
        "                    wave_points.append(data.index[i])\n",
        "    return wave_points\n",
        "\n",
        "# Create Additional Features\n",
        "def create_features(data):\n",
        "    \"\"\"Creates additional features like price change, log returns, and volatility.\"\"\"\n",
        "    if data is not None:\n",
        "        data['Price_Change'] = data['Close'].pct_change()\n",
        "        data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "        data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "        return data\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Calculate Price Channel\n",
        "def calculate_price_channel(data, window=20):\n",
        "    \"\"\"Calculates price channel (upper, lower, mid).\"\"\"\n",
        "    if data is not None:\n",
        "      data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "      data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "      data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "      return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "# Advanced Risk Management\n",
        "def advanced_risk_management(data):\n",
        "    \"\"\"Calculates ATR and sets stop loss and take profit levels.\"\"\"\n",
        "    if data is not None:\n",
        "        data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "        data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)\n",
        "        data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)\n",
        "        return data\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "\n",
        "# Build Strategy\n",
        "def build_strategy(data):\n",
        "    \"\"\"Builds the complete trading strategy.\"\"\"\n",
        "    if data is None:\n",
        "        print(\"No data to build strategy.\")\n",
        "        return None, None, None\n",
        "\n",
        "    data = clean_data(data)\n",
        "\n",
        "    if data is None:\n",
        "      return None, None, None\n",
        "\n",
        "    # Calculate technical indicators and features\n",
        "    order_blocks = calculate_order_blocks(data)\n",
        "    bos = calculate_bos(data)\n",
        "    choch = calculate_choch(data)\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    if support is not None and resistance is not None:\n",
        "      data['Support'] = support\n",
        "      data['Resistance'] = resistance\n",
        "\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "    data = calculate_rsi(data)\n",
        "    data = calculate_moving_average(data)\n",
        "    data = calculate_momentum(data)\n",
        "    data = analyze_volume(data)\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "    data = create_features(data)\n",
        "    data = calculate_price_channel(data)\n",
        "    data = advanced_risk_management(data)\n",
        "\n",
        "    # Print results for technical analysis\n",
        "    print(\"Order Blocks:\", order_blocks)\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    print(\"Wolf Wave Points:\", wave_points)\n",
        "\n",
        "    # Fetch and analyze sentiment from news\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    if sentiments:\n",
        "        data['Sentiment'] = np.mean(sentiments)\n",
        "    else:\n",
        "        data['Sentiment'] = 0 # Assign a default value if no sentiment found\n",
        "\n",
        "    # Prepare data for machine learning\n",
        "    data['target'] = data['Close'].shift(-1)  # shift target column up by one row\n",
        "    data.dropna(inplace=True) # Remove rows that got NaN because of the shift\n",
        "    features = data.drop(['target'], axis=1)\n",
        "    target = data['target']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Machine Learning models\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # Generate trading signals from models\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n",
        "    print(\"Strategy built successfully.\")\n",
        "    return signals_xgb, signals_rf, signals_svr\n",
        "\n",
        "# Analyze Results\n",
        "def analyze_results(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"Analyzes results and plots graphs.\"\"\"\n",
        "    if data is not None:\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        plt.plot(data.index, data['Close'], label='Close Price', color='blue')\n",
        "        if signals_xgb is not None:\n",
        "          plt.plot(data.index[-len(signals_xgb):], signals_xgb, label='XGBoost Signals', color='orange')\n",
        "        if signals_rf is not None:\n",
        "          plt.plot(data.index[-len(signals_rf):], signals_rf, label='Random Forest Signals', color='green')\n",
        "        if signals_svr is not None:\n",
        "          plt.plot(data.index[-len(signals_svr):], signals_svr, label='SVM Signals', color='red')\n",
        "        if 'Channel_Upper' in data.columns and 'Channel_Lower' in data.columns:\n",
        "            plt.plot(data.index, data['Channel_Upper'], label='Upper Channel', linestyle='--', color='purple')\n",
        "            plt.plot(data.index, data['Channel_Lower'], label='Lower Channel', linestyle='--', color='brown')\n",
        "        plt.title('Gold Price and Model Predictions')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Price / Signals')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        print(\"Results analyzed and plotted successfully.\")\n",
        "\n",
        "# Manage Risk\n",
        "def risk_management(data, signals):\n",
        "    \"\"\"Calculates and prints the maximum drawdown.\"\"\"\n",
        "    if data is not None and 'Returns' in data.columns:\n",
        "        max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "        print(f'Max Drawdown: {max_drawdown}')\n",
        "        print(\"Risk managed successfully.\")\n",
        "\n",
        "# Execute Trades\n",
        "def execute_trades(data, signals):\n",
        "    \"\"\"Executes trades based on the signals.\"\"\"\n",
        "    if data is not None and signals is not None:\n",
        "        for i, signal in enumerate(signals):\n",
        "            if signal > 0:\n",
        "                print(f\"Buy at {data.index[-len(signals) + i]} at price {data['Close'][-len(signals) + i]}\")\n",
        "                if 'Stop_Loss' in data.columns and 'Take_Profit' in data.columns:\n",
        "                  print(f\"Stop loss at {data['Stop_Loss'][-len(signals) + i]} and take profit at {data['Take_Profit'][-len(signals) + i]}\")\n",
        "            elif signal < 0:\n",
        "                print(f\"Sell at {data.index[-len(signals) + i]} at price {data['Close'][-len(signals) + i]}\")\n",
        "                if 'Stop_Loss' in data.columns and 'Take_Profit' in data.columns:\n",
        "                  print(f\"Stop loss at {data['Stop_Loss'][-len(signals) + i]} and take profit at {data['Take_Profit'][-len(signals) + i]}\")\n",
        "        print(\"Trades executed successfully.\")\n",
        "\n",
        "# Periodic Improvement (placeholder)\n",
        "def periodic_improvement(data):\n",
        "    \"\"\"Placeholder for periodic system improvements.\"\"\"\n",
        "    # Add logic for re-training the model and updating the strategy here\n",
        "    print(\"Periodic improvement check completed.\")\n",
        "    pass\n",
        "\n",
        "# Save Results to CSV\n",
        "def save_results_to_csv(data, filename='results.csv'):\n",
        "    \"\"\"Saves the results in a CSV file.\"\"\"\n",
        "    if data is not None:\n",
        "      data.to_csv(filename)\n",
        "      print(\"Results saved to CSV successfully.\")\n",
        "\n",
        "# Analyze All Models\n",
        "def analyze_all_models(data, signals_xgb, signals_rf, signals_svr):\n",
        "  \"\"\"Analyzes the total return of each model.\"\"\"\n",
        "  print(\"Analyzing performance of all models:\")\n",
        "  if signals_xgb is not None:\n",
        "    print(f'Total Return XGBoost: {signals_xgb.sum()}')\n",
        "  if signals_rf is not None:\n",
        "    print(f'Total Return Random Forest: {signals_rf.sum()}')\n",
        "  if signals_svr is not None:\n",
        "    print(f'Total Return SVM: {signals_svr.sum()}')\n",
        "\n",
        "# Risk Analysis\n",
        "def risk_analysis(data):\n",
        "    \"\"\"Analyzes and prints max drawdown of the strategy.\"\"\"\n",
        "    if data is not None and 'Returns' in data.columns:\n",
        "      max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "      print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# Correlation Analysis\n",
        "def correlation_analysis(data):\n",
        "    \"\"\"Analyzes and plots the correlation matrix of the features.\"\"\"\n",
        "    if data is not None:\n",
        "        correlation = data.corr()\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "        plt.title('Correlation Matrix')\n",
        "        plt.show()\n",
        "\n",
        "# Time Series Analysis\n",
        "def time_series_analysis(data):\n",
        "    \"\"\"Performs and plots time series decomposition.\"\"\"\n",
        "    if data is not None:\n",
        "      seasonal_decomp = seasonal_decompose(data['Close'], model='additive', period=30)\n",
        "      seasonal_decomp.plot()\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "# Train Neural Network\n",
        "def train_neural_network(X_train, y_train):\n",
        "    \"\"\"Trains the neural network model.\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))  # Output layer\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "    print(\"Neural network trained successfully.\")\n",
        "    return model\n",
        "\n",
        "# Generate Signals with the Neural Network\n",
        "def generate_signals(model, data):\n",
        "  \"\"\"Generates signals using a trained model.\"\"\"\n",
        "  if model is not None and data is not None:\n",
        "    return model.predict(data)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "# Evaluate Neural Network Performance\n",
        "def evaluate_performance(signals, data):\n",
        "    \"\"\"Evaluates and prints the total return from the signals.\"\"\"\n",
        "    if data is not None and 'Returns' in data.columns:\n",
        "        total_return = data['Returns'].sum()\n",
        "        print(f'Total Return: {total_return}')\n",
        "        print(\"Neural network performance evaluated.\")\n",
        "\n",
        "# Compare Models\n",
        "def compare_models(signals_xgb, nn_signals, data):\n",
        "    \"\"\"Compares cumulative returns of XGBoost and neural network models.\"\"\"\n",
        "    if data is not None and signals_xgb is not None and nn_signals is not None:\n",
        "      plt.figure(figsize=(14, 7))\n",
        "      plt.plot(data.index[-len(signals_xgb):], np.cumsum(signals_xgb), label='XGBoost Cumulative Returns', color='blue')\n",
        "      plt.plot(data.index[-len(nn_signals):], np.cumsum(nn_signals), label='Neural Network Cumulative Returns', color='orange')\n",
        "      plt.title('Model Comparison: XGBoost vs Neural Network')\n",
        "      plt.xlabel('Date')\n",
        "      plt.ylabel('Cumulative Returns')\n",
        "      plt.axhline(0, color='red', linestyle='--')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "\n",
        "# Historical Analysis\n",
        "def historical_analysis(data):\n",
        "    \"\"\"Analyzes and plots distribution of historical returns.\"\"\"\n",
        "    if data is not None:\n",
        "        historical_returns = data['Close'].pct_change().dropna()\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        plt.hist(historical_returns, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "        plt.title('Historical Returns Distribution')\n",
        "        plt.xlabel('Returns')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "        print(\"Historical analysis completed.\")\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    if data is not None:\n",
        "      data['Returns'] = data['Close'].pct_change()\n",
        "\n",
        "    # Build the trading strategy\n",
        "    signals_xgb, signals_rf, signals_svr = build_strategy(data)\n",
        "\n",
        "    # Analyze the results and plot graphs\n",
        "    analyze_results(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "    # Manage risk\n",
        "    if signals_xgb is not None:\n",
        "      risk_management(data, signals_xgb)\n",
        "\n",
        "    # Execute trades based on XGBoost model signals\n",
        "    if signals_xgb is not None:\n",
        "      execute_trades(data, signals_xgb)\n",
        "\n",
        "    # Perform periodic improvements (placeholder)\n",
        "    periodic_improvement(data)\n",
        "\n",
        "    # Save the results in CSV format\n",
        "    save_results_to_csv(data)\n",
        "\n",
        "    # Analyze all model\n",
        "    analyze_all_models(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "    # Perform risk analysis\n",
        "    risk_analysis(data)\n",
        "\n",
        "    # Perform Correlation Analysis\n",
        "    correlation_analysis(data)\n",
        "\n",
        "    # Perform Time Series Analysis\n",
        "    time_series_analysis(data)\n",
        "\n",
        "    # Split data for the neural network\n",
        "    if data is not None:\n",
        "      X_train, X_test, y_train, y_test = train_test_split(data.drop(['target'], axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "      # Train the neural network model\n",
        "      nn_model = train_neural_network(X_train, y_train)\n",
        "    else:\n",
        "      nn_model = None\n",
        "\n",
        "\n",
        "    # Generate signals using the neural network\n",
        "    nn_signals = generate_signals(nn_model, X_test)\n",
        "\n",
        "\n",
        "    # Evaluate performance of the neural network\n",
        "    if nn_signals is not None:\n",
        "      evaluate_performance(nn_signals, data)\n",
        "\n",
        "    # Compare performance between XGBoost and neural network models\n",
        "    if nn_signals is not None:\n",
        "      compare_models(signals_xgb, nn_signals, data)\n",
        "\n",
        "    # Perform periodic improvements\n",
        "    periodic_improvement(data)\n",
        "\n",
        "    # Historical Data Analysis\n",
        "    historical_analysis(data)\n",
        "\n",
        "    print(\"Strategy completed.\")"
      ],
      "metadata": {
        "id": "9TIugiUTfaPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from textblob import TextBlob\n",
        "import pandas_ta as ta\n",
        "\n",
        "# تحميل البيانات\n",
        "def load_data(filename):\n",
        "    \"\"\"تحميل البيانات من ملف CSV\"\"\"\n",
        "    data = pd.read_csv(filename, index_col='date', parse_dates=True)\n",
        "    return data\n",
        "\n",
        "# تنظيف البيانات\n",
        "def clean_data(data):\n",
        "    \"\"\"تنظيف البيانات وإزالة القيم المفقودة\"\"\"\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "# جمع الأخبار من [Investing.com](http://investing.com/)\n",
        "def fetch_news():\n",
        "    \"\"\"جمع الأخبار من موقع [Investing.com](http://investing.com/)\"\"\"\n",
        "    url = \"https://www.investing.com/news/commodities-news/gold\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    news_headers = soup.select('div.news-item-header')\n",
        "    news_titles = [header.text.strip() for header in news_headers]\n",
        "    return news_titles\n",
        "\n",
        "# تحليل المشاعر من الأخبار باستخدام TextBlob\n",
        "def analyze_sentiment(news_titles):\n",
        "    \"\"\"تحليل المشاعر من العناوين باستخدام TextBlob\"\"\"\n",
        "    sentiments = []\n",
        "    for title in news_titles:\n",
        "        analysis = TextBlob(title)\n",
        "        score = analysis.sentiment.polarity  # تحليل المشاعر\n",
        "        sentiments.append(score)\n",
        "    return sentiments\n",
        "\n",
        "# حساب Order Blocks\n",
        "def calculate_order_blocks(data):\n",
        "    \"\"\"حساب مناطق Order Blocks\"\"\"\n",
        "    order_blocks = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]:\n",
        "            order_blocks.append((data.index[i-1], data['Close'][i-1]))\n",
        "    return order_blocks\n",
        "\n",
        "# حساب Break of Structure (BOS)\n",
        "def calculate_bos(data):\n",
        "    \"\"\"حساب مناطق BOS\"\"\"\n",
        "    bos = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "            bos.append(data.index[i])\n",
        "    return bos\n",
        "\n",
        "# حساب Change of Character (CHOCH)\n",
        "def calculate_choch(data):\n",
        "    \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "    choch = []\n",
        "    for i in range(1, len(data)):\n",
        "        if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "           (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "            choch.append(data.index[i])\n",
        "    return choch\n",
        "\n",
        "# حساب مناطق العرض والطلب\n",
        "def calculate_supply_demand(data):\n",
        "    \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "    supply_zones = []\n",
        "    demand_zones = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['High'][i-1]:\n",
        "            supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "        elif data['Close'][i] < data['Low'][i-1]:\n",
        "            demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "    return supply_zones, demand_zones\n",
        "\n",
        "# حساب خطوط الدعم والمقاومة\n",
        "def calculate_support_resistance(data):\n",
        "    \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "    support = data['Low'].rolling(window=20).min()\n",
        "    resistance = data['High'].rolling(window=20).max()\n",
        "    return support, resistance\n",
        "\n",
        "# حساب مستويات فيبوناتشي\n",
        "def fibonacci_retracement(high, low):\n",
        "    \"\"\"حساب مستويات فيبوناتشي\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n",
        "\n",
        "# حساب مؤشر القوة النسبية (RSI) باستخدام pandas_ta\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"حساب RSI\"\"\"\n",
        "    data['RSI'] = ta.rsi(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب المتوسط المتحرك باستخدام pandas_ta\n",
        "def calculate_moving_average(data, period=20):\n",
        "    \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "    data['MA'] = ta.sma(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب الزخم (Momentum) باستخدام pandas_ta\n",
        "def calculate_momentum(data, period=10):\n",
        "    \"\"\"حساب الزخم\"\"\"\n",
        "    data['Momentum'] = ta.ema(data['Close'], length=period) - data['Close']\n",
        "    return data\n",
        "\n",
        "# تحليل حجم التداول\n",
        "def analyze_volume(data):\n",
        "    \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    return data\n",
        "\n",
        "# حساب موجة الذئب\n",
        "def calculate_wolf_wave(data):\n",
        "    \"\"\"حساب موجة الذئب\"\"\"\n",
        "    wave_points = []\n",
        "    for i in range(1, len(data)-4):\n",
        "        if (data['Close'][i] < data['Close'][i-1] and\n",
        "            data['Close'][i+1] < data['Close'][i] and\n",
        "            data['Close'][i+2] > data['Close'][i+1] and\n",
        "            data['Close'][i+3] < data['Close'][i+2] and\n",
        "            data['Close'][i+4] > data['Close'][i+3]):\n",
        "            wave_points.append(data.index[i])\n",
        "    return wave_points\n",
        "\n",
        "# إنشاء ميزات إضافية\n",
        "def create_features(data):\n",
        "    \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "    data['Price_Change'] = data['Close'].pct_change()\n",
        "    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "    return data\n",
        "\n",
        "# حساب مؤشر القنوات السعرية\n",
        "def calculate_price_channel(data, window=20):\n",
        "    \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "    data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "    data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "    data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "    return data\n",
        "\n",
        "# تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "def advanced_risk_management(data):\n",
        "    \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "    # حساب ATR (Average True Range) باستخدام pandas_ta\n",
        "    data['ATR'] = ta.atr(data['High'], data['Low'], data['Close'], length=14)\n",
        "\n",
        "    # تحديد مستويات وقف الخسارة\n",
        "    data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "    data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "    return data\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "def build_strategy(data):\n",
        "    \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "    data = clean_data(data)\n",
        "\n",
        "    # حساب Order Blocks وBOS وCHOCH\n",
        "    order_blocks = calculate_order_blocks(data)\n",
        "    bos = calculate_bos(data)\n",
        "    choch = calculate_choch(data)\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    data['Support'] = support\n",
        "    data['Resistance'] = resistance\n",
        "\n",
        "    # حساب مستويات فيبوناتشي\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "\n",
        "    # حساب RSI\n",
        "    data = calculate_rsi(data)\n",
        "\n",
        "    # حساب Moving Average\n",
        "    data = calculate_moving_average(data)\n",
        "\n",
        "    # حساب Momentum\n",
        "    data = calculate_momentum(data)\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    data = analyze_volume(data)\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    data = create_features(data)\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    data = calculate_price_channel(data)\n",
        "\n",
        "    # تحسين إدارة المخاطر\n",
        "    data = advanced_risk_management(data)\n",
        "\n",
        "    # عرض النتائج\n",
        "    print(\"Order Blocks:\", order_blocks)\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    print(\"Wolf Wave Points:\", wave_points)\n",
        "\n",
        "    # تحليل المشاعر\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    data['Sentiment'] = np.mean(sentiments)\n",
        "\n",
        "    # تحديد الميزات والهدف\n",
        "    features = data.drop(['target'], axis=1)\n",
        "    target = data['target']\n",
        "\n",
        "    # تقسيم البيانات\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # نموذج XGBoost\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج Random Forest\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج SVM\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # توليد الإشارات\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n",
        "    return signals_xgb, signals_rf, signals_svr, data\n",
        "\n",
        "# تحليل النتائج\n",
        "def analyze_results(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل النتائج ورسم الرسوم البيانية\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # رسم سعر الإغلاق\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(data.index, data['Close'], label='سعر الإغلاق', color='blue')\n",
        "    plt.title('سعر الذهب')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('السعر')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم إشارات XGBoost\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(data.index[-len(signals_xgb):], signals_xgb, label='إشارات XGBoost', color='orange')\n",
        "    plt.title('إشارات XGBoost')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('الإشارات')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم إشارات Random Forest\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(data.index[-len(signals_rf):], signals_rf, label='إشارات Random Forest', color='green')\n",
        "    plt.title('إشارات Random Forest')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('الإشارات')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# إدارة المخاطر\n",
        "def risk_management(data, signals):\n",
        "    \"\"\"إدارة المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "def execute_trades(data, signals):\n",
        "    \"\"\"تنفيذ التداولات بناءً على الإشارات\"\"\"\n",
        "    for i, signal in enumerate(signals):\n",
        "        if signal > 0:\n",
        "            print(f\"شراء عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "        elif signal < 0:\n",
        "            print(f\"بيع عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "def periodic_improvement(data):\n",
        "    \"\"\"تحسين النظام بشكل دوري\"\"\"\n",
        "    # تحليل الأداء بشكل دوري\n",
        "    pass\n",
        "\n",
        "# إضافة دالة لتحزين النتائج\n",
        "def save_results_to_csv(data, filename='results.csv'):\n",
        "    \"\"\"تخزين النتائج في ملف CSV\"\"\"\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "# إضافة تحليل شامل لجميع النماذج\n",
        "def analyze_all_models(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل شامل لجميع النماذج\"\"\"\n",
        "    print(\"تحليل أداء النماذج:\")\n",
        "    print(f'Total Return XGBoost: {signals_xgb.sum()}')\n",
        "    print(f'Total Return Random Forest: {signals_rf.sum()}')\n",
        "    print(f'Total Return SVM: {signals_svr.sum()}')\n",
        "\n",
        "# إضافة تحليل المخاطر\n",
        "def risk_analysis(data):\n",
        "    \"\"\"تحليل المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# إضافة تحليل الارتباط\n",
        "def correlation_analysis(data):\n",
        "    \"\"\"تحليل الارتباط بين الميزات المختلفة\"\"\"\n",
        "    correlation = data.corr()\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.title('مصفوفة الارتباط')\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحليل زمني\n",
        "def time_series_analysis(data):\n",
        "    \"\"\"تحليل السلاسل الزمنية\"\"\"\n",
        "    seasonal_decomp = seasonal_decompose(data['Close'], model='additive', period=30)\n",
        "    seasonal_decomp.plot()\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحسينات على نماذج التعلم الآلي\n",
        "def train_neural_network(X_train, y_train):\n",
        "    \"\"\"تدريب الشبكة العصبية\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))  # الخرج النهائي\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "    return model\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "data = load_data('gold_price_data.csv')\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "signals_xgb, signals_rf, signals_svr, data = build_strategy(data)\n",
        "\n",
        "# تحليل النتائج\n",
        "analyze_results(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# إدارة المخاطر\n",
        "risk_management(data, signals_xgb)\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "execute_trades(data, signals_xgb)\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "periodic_improvement(data)\n",
        "\n",
        "# تخزين النتائج\n",
        "save_results_to_csv(data)\n",
        "\n",
        "# تحليل شامل لجميع النماذج\n",
        "analyze_all_models(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# تحليل المخاطر\n",
        "risk_analysis(data)\n",
        "\n",
        "# تحليل الارتباط\n",
        "correlation_analysis(data)\n",
        "\n",
        "# تحليل زمني\n",
        "time_series_analysis(data)\n",
        "\n",
        "# تدريب الشبكة العصبية\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop(['target'], axis=1), data['target'], test_size=0.2, random_state=42)\n",
        "nn_model = train_neural_network(X_train, y_train)\n",
        "\n",
        "# تنفيذ الاستراتيجية باستخدام الشبكة العصبية\n",
        "def generate_signals(model, data):\n",
        "    \"\"\"توليد الإشارات باستخدام النموذج المدرب\"\"\"\n",
        "    return model.predict(data)\n",
        "\n",
        "nn_signals = generate_signals(nn_model, X_test)\n",
        "\n",
        "# تقييم أداء الشبكة العصبية\n",
        "def evaluate_performance(signals, data):\n",
        "    \"\"\"تقييم أداء النموذج\"\"\"\n",
        "    total_return = data['Returns'].sum()\n",
        "    print(f'Total Return: {total_return}')\n",
        "\n",
        "evaluate_performance(nn_signals, data)\n",
        "\n",
        "# مقارنة النماذج\n",
        "def compare_models(signals_xgb, nn_signals):\n",
        "    \"\"\"مقارنة أداء النماذج المختلفة\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(data.index, signals_xgb.cumsum(), label='عائد XGBoost التراكمي', color='blue')\n",
        "    plt.plot(data.index, nn_signals.cumsum(), label='عائد الشبكة العصبية التراكمي', color='orange')\n",
        "    plt.title('مقارنة النماذج: XGBoost مقابل الشبكة العصبية')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('العائد التراكمي')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# مقارنة النماذج\n",
        "compare_models(signals_xgb, nn_signals)\n"
      ],
      "metadata": {
        "id": "aPtPKnLTXiCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت المكتبات المطلوبة\n",
        "!pip install pandas_ta gdown keras xgboost seaborn statsmodels textblob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_ta as ta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from textblob import TextBlob\n",
        "import gdown\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# تحميل البيانات من Google Drive\n",
        "def load_data_from_drive(file_id='YOUR_FILE_ID'):  # استبدل YOUR_FILE_ID بمعرف ملفك\n",
        "    \"\"\"تحميل البيانات من Google Drive باستخدام gdown.\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        file_path = '/content/drive/My Drive/gold_usd_prices.csv'\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(\"First 5 rows of the DataFrame:\")\n",
        "        print(data.head())\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from Drive: {e}\")\n",
        "        return None\n",
        "\n",
        "# تنظيف البيانات\n",
        "def clean_data(data):\n",
        "    \"\"\"تنظيف البيانات وإزالة القيم المفقودة.\"\"\"\n",
        "    data.dropna(inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# جمع الأخبار من Investing.com\n",
        "def fetch_news():\n",
        "    \"\"\"جمع الأخبار من موقع Investing.com\"\"\"\n",
        "    url = \"https://www.investing.com/news/commodities-news/gold\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    news_headers = soup.select('div.news-item-header')\n",
        "    news_titles = [header.text.strip() for header in news_headers]\n",
        "\n",
        "    return news_titles\n",
        "\n",
        "# تحليل المشاعر من الأخبار باستخدام TextBlob\n",
        "def analyze_sentiment(news_titles):\n",
        "    \"\"\"تحليل المشاعر من العناوين باستخدام TextBlob\"\"\"\n",
        "    sentiments = []\n",
        "    for title in news_titles:\n",
        "        analysis = TextBlob(title)\n",
        "        score = analysis.sentiment.polarity  # تحليل المشاعر\n",
        "        sentiments.append(score)\n",
        "    return sentiments\n",
        "\n",
        "#حساب اوردر بلوك\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def احسب_وارسم_مناطق_طلبات_الشراء_والبيع(البيانات):\n",
        "    \"\"\"\n",
        "    يحسب ويرسم مناطق طلبات الشراء والبيع (Order Blocks) على الرسم البياني.\n",
        "\n",
        "    المعطيات:\n",
        "        البيانات: DataFrame بيانات السعر.\n",
        "    \"\"\"\n",
        "\n",
        "    # حساب مناطق طلبات الشراء\n",
        "    مناطق_طلبات_الشراء = []\n",
        "    for i in range(2, len(البيانات)):\n",
        "        if البيانات['Close'].iloc[i] < البيانات['Close'].iloc[i-1] and البيانات['Close'].iloc[i-1] > البيانات['Close'].iloc[i-2]:\n",
        "            مناطق_طلبات_الشراء.append((البيانات.index[i-1], البيانات['Close'].iloc[i-1]))\n",
        "\n",
        "    # حساب مناطق طلبات البيع\n",
        "    مناطق_طلبات_البيع = []\n",
        "    for i in range(2, len(البيانات)):\n",
        "        if البيانات['Close'].iloc[i] > البيانات['Close'].iloc[i-1] and البيانات['Close'].iloc[i-1] < البيانات['Close'].iloc[i-2]:\n",
        "            مناطق_طلبات_البيع.append((البيانات.index[i-1], البيانات['Close'].iloc[i-1]))\n",
        "\n",
        "    # رسم الرسم البياني\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(البيانات.index, البيانات['Close'], label='سعر الإغلاق', color='blue')\n",
        "\n",
        "    # رسم مناطق طلبات الشراء\n",
        "    for منطقة in مناطق_طلبات_الشراء:\n",
        "        index, price = منطقة\n",
        "        plt.plot(index, price, marker='o', markersize=8, color='red', label='طلب شراء' if منطقة == مناطق_طلبات_الشراء[0] else \"\")\n",
        "\n",
        "    # رسم مناطق طلبات البيع\n",
        "    for منطقة in مناطق_طلبات_البيع:\n",
        "        index, price = منطقة\n",
        "        plt.plot(index, price, marker='o', markersize=8, color='green', label='طلب بيع' if منطقة == مناطق_طلبات_البيع[0] else \"\")\n",
        "\n",
        "    plt.title('مناطق طلبات الشراء والبيع على الرسم البياني')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('السعر')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# حساب Break of Structure (BOS)\n",
        "def calculate_bos(data):\n",
        "    \"\"\"حساب مناطق BOS\"\"\"\n",
        "    bos = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "            bos.append(data.index[i])\n",
        "    return bos\n",
        "\n",
        "# حساب Change of Character (CHOCH)\n",
        "def calculate_choch(data):\n",
        "    \"\"\"حساب مناطق CHOCH\"\"\"\n",
        "    choch = []\n",
        "    for i in range(1, len(data)):\n",
        "        if (data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]) or \\\n",
        "           (data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]):\n",
        "            choch.append(data.index[i])\n",
        "    return choch\n",
        "\n",
        "# حساب مناطق العرض والطلب\n",
        "def calculate_supply_demand(data):\n",
        "    \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "    supply_zones = []\n",
        "    demand_zones = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['High'][i-1]:\n",
        "            supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "        elif data['Close'][i] < data['Low'][i-1]:\n",
        "            demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "    return supply_zones, demand_zones\n",
        "\n",
        "# حساب خطوط الدعم والمقاومة\n",
        "def calculate_support_resistance(data):\n",
        "    \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "    support = data['Low'].rolling(window=20).min()\n",
        "    resistance = data['High'].rolling(window=20).max()\n",
        "    return support, resistance\n",
        "\n",
        "# حساب مستويات فيبوناتشي\n",
        "def fibonacci_retracement(high, low):\n",
        "    \"\"\"حساب مستويات فيبوناتشي\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n",
        "\n",
        "# حساب مؤشر القوة النسبية (RSI) باستخدام Pandas TA\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"حساب RSI\"\"\"\n",
        "    data['RSI'] = ta.rsi(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب المتوسط المتحرك باستخدام Pandas TA\n",
        "def calculate_moving_average(data, period=20):\n",
        "    \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "    data['MA'] = ta.sma(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب الزخم (Momentum) باستخدام Pandas TA\n",
        "def calculate_momentum(data, period=10):\n",
        "    \"\"\"حساب الزخم\"\"\"\n",
        "    data['Momentum'] = ta.mom(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# تحليل حجم التداول\n",
        "def analyze_volume(data):\n",
        "    \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    return data\n",
        "\n",
        "# حساب موجة الذئب\n",
        "def calculate_wolf_wave(data):\n",
        "    \"\"\"حساب موجة الذئب\"\"\"\n",
        "    wave_points = []\n",
        "    for i in range(1, len(data)-4):\n",
        "        if (data['Close'][i] < data['Close'][i-1] and\n",
        "            data['Close'][i+1] < data['Close'][i] and\n",
        "            data['Close'][i+2] > data['Close'][i+1] and\n",
        "            data['Close'][i+3] < data['Close'][i+2] and\n",
        "            data['Close'][i+4] > data['Close'][i+3]):\n",
        "            wave_points.append(data.index[i])\n",
        "    return wave_points\n",
        "\n",
        "# إنشاء ميزات إضافية\n",
        "def create_features(data):\n",
        "    \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "    data['Price_Change'] = data['Close'].pct_change()\n",
        "    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "    return data\n",
        "\n",
        "# حساب مؤشر القنوات السعرية\n",
        "def calculate_price_channel(data, window=20):\n",
        "    \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "    data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "    data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "    data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "    return data\n",
        "\n",
        "# تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "def advanced_risk_management(data):\n",
        "    \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "    # حساب ATR (Average True Range)\n",
        "    data['ATR'] = ta.atr(data['High'], data['Low'], data['Close'], length=14)\n",
        "\n",
        "    # تحديد مستويات وقف الخسارة\n",
        "    data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "    data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "    return data\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "def build_strategy(data):\n",
        "    \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "    data = clean_data(data)\n",
        "\n",
        "    # حساب Order Blocks وBOS وCHOCH\n",
        "    order_blocks = calculate_order_blocks(data)\n",
        "    bos = calculate_bos(data)\n",
        "    choch = calculate_choch(data)\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    data['Support'] = support\n",
        "    data['Resistance'] = resistance\n",
        "\n",
        "    # حساب مستويات فيبوناتشي\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "\n",
        "    # حساب RSI\n",
        "    data = calculate_rsi(data)\n",
        "\n",
        "    # حساب Moving Average\n",
        "    data = calculate_moving_average(data)\n",
        "\n",
        "    # حساب Momentum\n",
        "    data = calculate_momentum(data)\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    data = analyze_volume(data)\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    data = create_features(data)\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    data = calculate_price_channel(data)\n",
        "\n",
        "    # تحسين إدارة المخاطر\n",
        "    data = advanced_risk_management(data)\n",
        "\n",
        "    # عرض النتائج\n",
        "    print(\"Order Blocks:\", order_blocks)\n",
        "    print(\"BOS:\", bos)\n",
        "    print(\"CHOCH:\", choch)\n",
        "    print(\"Supply Zones:\", supply_zones)\n",
        "    print(\"Demand Zones:\", demand_zones)\n",
        "    print(\"Wolf Wave Points:\", wave_points)\n",
        "\n",
        "    # تحليل المشاعر\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    data['Sentiment'] = np.mean(sentiments)\n",
        "\n",
        "    # رسم بياني لكل فقرة\n",
        "    plot_data(data)\n",
        "\n",
        "    # تحديد الميزات والهدف\n",
        "    features = data.drop(['target'], axis=1)\n",
        "    target = data['target']\n",
        "\n",
        "    # تقسيم البيانات\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # نموذج XGBoost\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج Random Forest\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج SVM\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # توليد الإشارات\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n",
        "    return signals_xgb, signals_rf, signals_svr\n",
        "\n",
        "# رسم البيانات\n",
        "def plot_data(data):\n",
        "    \"\"\"رسم البيانات\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    # رسم الأسعار\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(data.index, data['Close'], label='Close Price', color='blue')\n",
        "    plt.title('Close Price')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم RSI\n",
        "    plt.subplot(3, 2, 2)\n",
        "    plt.plot(data.index, data['RSI'], label='RSI', color='orange')\n",
        "    plt.axhline(70, linestyle='--', color='red')\n",
        "    plt.axhline(30, linestyle='--', color='green')\n",
        "    plt.title('Relative Strength Index (RSI)')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('RSI Value')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم المتوسط المتحرك\n",
        "    plt.subplot(3, 2, 3)\n",
        "    plt.plot(data.index, data['Close'], label='Close Price', color='blue')\n",
        "    plt.plot(data.index, data['MA'], label='Moving Average', color='orange')\n",
        "    plt.title('Close Price and Moving Average')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم الزخم\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.plot(data.index, data['Momentum'], label='Momentum', color='green')\n",
        "    plt.title('Momentum')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Momentum Value')\n",
        "    plt.legend()\n",
        "\n",
        "    # رسم حجم التداول\n",
        "    plt.subplot(3, 2, 5)\n",
        "    plt.bar(data.index, data['Volume'], label='Volume', color='purple')\n",
        "    plt.plot(data.index, data['Volume_MA'], label='Volume MA', color='orange')\n",
        "    plt.title('Volume and Volume Moving Average')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Volume')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# تحليل النتائج\n",
        "def analyze_results(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل النتائج ورسم الرسوم البيانية\"\"\"\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(data.index, data['Close'], label='Close Price', color='blue')\n",
        "    plt.plot(data.index[-len(signals_xgb):], signals_xgb, label='XGBoost Signals', color='orange')\n",
        "    plt.plot(data.index[-len(signals_rf):], signals_rf, label='Random Forest Signals', color='green')\n",
        "    plt.plot(data.index[-len(signals_svr):], signals_svr, label='SVM Signals', color='red')\n",
        "    plt.plot(data.index, data['Channel_Upper'], label='Upper Channel', linestyle='--', color='purple')\n",
        "    plt.plot(data.index, data['Channel_Lower'], label='Lower Channel', linestyle='--', color='brown')\n",
        "    plt.title('Gold Price and Model Predictions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Price / Signals')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# إدارة المخاطر\n",
        "def risk_management(data):\n",
        "    \"\"\"إدارة المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "def execute_trades(data, signals):\n",
        "    \"\"\"تنفيذ التداولات بناءً على الإشارات\"\"\"\n",
        "    for i, signal in enumerate(signals):\n",
        "        if signal > 0:\n",
        "            print(f\"شراء عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "        elif signal < 0:\n",
        "            print(f\"بيع عند {data.index[i]} بسعر {data['Close'][i]}\")\n",
        "            print(f\"وقف خسارة عند {data['Stop_Loss'][i]} وجني أرباح عند {data['Take_Profit'][i]}\")\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "def periodic_improvement(data):\n",
        "    \"\"\"تحسين النظام بشكل دوري\"\"\"\n",
        "    # تحليل الأداء بشكل دوري\n",
        "    pass\n",
        "\n",
        "# إضافة دالة لتحزين النتائج\n",
        "def save_results_to_csv(data, filename='results.csv'):\n",
        "    \"\"\"تخزين النتائج في ملف CSV\"\"\"\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "# إضافة تحليل شامل لجميع النماذج\n",
        "def analyze_all_models(data, signals_xgb, signals_rf, signals_svr):\n",
        "    \"\"\"تحليل شامل لجميع النماذج\"\"\"\n",
        "    print(\"تحليل أداء النماذج:\")\n",
        "    print(f'Total Return XGBoost: {signals_xgb.sum()}')\n",
        "    print(f'Total Return Random Forest: {signals_rf.sum()}')\n",
        "    print(f'Total Return SVM: {signals_svr.sum()}')\n",
        "\n",
        "# إضافة تحليل المخاطر\n",
        "def risk_analysis(data):\n",
        "    \"\"\"تحليل المخاطر\"\"\"\n",
        "    max_drawdown = (data['Returns'].cumsum().min() - data['Returns'].cumsum().max()) / data['Returns'].cumsum().max()\n",
        "    print(f'Max Drawdown: {max_drawdown}')\n",
        "\n",
        "# إضافة تحليل الارتباط\n",
        "def correlation_analysis(data):\n",
        "    \"\"\"تحليل الارتباط بين الميزات المختلفة\"\"\"\n",
        "    correlation = data.corr()\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحليل زمني\n",
        "def time_series_analysis(data):\n",
        "    \"\"\"تحليل السلاسل الزمنية\"\"\"\n",
        "    seasonal_decomp = seasonal_decompose(data['Close'], model='additive', period=30)\n",
        "    seasonal_decomp.plot()\n",
        "    plt.show()\n",
        "\n",
        "# إضافة تحسينات على نماذج التعلم الآلي\n",
        "def train_neural_network(X_train, y_train):\n",
        "    \"\"\"تدريب الشبكة العصبية\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))  # الخرج النهائي\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
        "    return model\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "file_id = 'YOUR_FILE_ID'  # ضع هنا معرف الملف من Google Drive\n",
        "data = load_data_from_drive(file_id)\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "signals_xgb, signals_rf, signals_svr = build_strategy(data)\n",
        "\n",
        "# تحليل النتائج\n",
        "analyze_results(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# إدارة المخاطر\n",
        "risk_management(data)\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "execute_trades(data, signals_xgb)\n",
        "\n",
        "# تحسين النظام بشكل دوري\n",
        "periodic_improvement(data)\n",
        "\n",
        "# تخزين النتائج\n",
        "save_results_to_csv(data)\n",
        "\n",
        "# تحليل شامل لجميع النماذج\n",
        "analyze_all_models(data, signals_xgb, signals_rf, signals_svr)\n",
        "\n",
        "# تحليل المخاطر\n",
        "risk_analysis(data)\n",
        "\n",
        "# تحليل الارتباط\n",
        "correlation_analysis(data)\n",
        "\n",
        "# تحليل زمني\n",
        "time_series_analysis(data)\n",
        "\n",
        "# تدريب الشبكة العصبية\n",
        "#X_train, X_test, y_train, y_test = train_test_split(data.drop(['target'], axis=1),\n"
      ],
      "metadata": {
        "id": "Gyvj26bzeRzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# تثبيت المكتبات المطلوبة\n",
        "!pip install pandas_ta gdown keras xgboost seaborn statsmodels textblob\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_ta as ta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from textblob import TextBlob\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# تحميل البيانات من Google Drive\n",
        "def load_and_clean_data(file_id='YOUR_FILE_ID'):\n",
        "    \"\"\"تحميل البيانات من Google Drive وتنظيفها.\"\"\"\n",
        "\n",
        "\n",
        "#def set_price_as_index_and_clean(df):\n",
        "    \"\"\"\n",
        "    تعيين عمود \"Price\" كفهرس وإزالة البيانات غير الرقمية في الصفوف التي تلي صف الفهرس.\n",
        "\n",
        "    Args:\n",
        "        df: إطار البيانات المدخل.\n",
        "\n",
        "    Returns:\n",
        "        إطار البيانات المعدل.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        file_path = '/content/drive/My Drive/gold_usd_prices.csv'\n",
        "        data = pd.read_csv(file_path, index_col='Price', parse_dates=True)\n",
        "        print(\"First 5 rows of the DataFrame:\")\n",
        "        print(data.head())\n",
        "\n",
        "        # تنظيف البيانات: إزالة القيم المفقودة\n",
        "        data.dropna(inplace=True)\n",
        "        print(\"\\nDataFrame after cleaning:\")\n",
        "        print(data.head())\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or cleaning data: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# جمع الأخبار من Investing.com\n",
        "def fetch_news():\n",
        "    \"\"\"جمع الأخبار من موقع Investing.com\"\"\"\n",
        "    url = \"https://www.investing.com/news/commodities-news/gold\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    news_headers = soup.select('div.news-item-header')\n",
        "    news_titles = [header.text.strip() for header in news_headers]\n",
        "\n",
        "    return news_titles\n",
        "\n",
        "# تحليل المشاعر من الأخبار باستخدام TextBlob\n",
        "def analyze_sentiment(news_titles):\n",
        "    \"\"\"تحليل المشاعر من العناوين باستخدام TextBlob\"\"\"\n",
        "    sentiments = []\n",
        "    for title in news_titles:\n",
        "        analysis = TextBlob(title)\n",
        "        score = analysis.sentiment.polarity  # تحليل المشاعر\n",
        "        sentiments.append(score)\n",
        "    return sentiments\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Call the functions and print the results\n",
        "news_titles = fetch_news()\n",
        "sentiments = analyze_sentiment(news_titles)\n",
        "\n",
        "print(\"News Titles:\")\n",
        "for title in news_titles:\n",
        "    print(title)\n",
        "\n",
        "print(\"\\nSentiment Scores:\")\n",
        "for score in sentiments:\n",
        "    print(score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# تحليل المفكرة الاقتصادية\n",
        "def fetch_economic_calendar():\n",
        "    \"\"\"جمع البيانات من المفكرة الاقتصادية\"\"\"\n",
        "    url = \"https://www.investing.com/economic-calendar/\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    events = []\n",
        "    for event in soup.select('table.ecTable tr'):\n",
        "        try:\n",
        "            date = event.select_one('td.date').text.strip()\n",
        "            impact = event.select_one('td.impact').text.strip()\n",
        "            title = event.select_one('td.event').text.strip()\n",
        "            events.append((date, impact, title))\n",
        "        except AttributeError:\n",
        "            continue\n",
        "\n",
        "    return events\n",
        "\n",
        "# حساب Order Blocks\n",
        "def calculate_order_blocks(data):\n",
        "    \"\"\"حساب مناطق Order Blocks\"\"\"\n",
        "    bullish_blocks = []\n",
        "    bearish_blocks = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['Close'][i-1] and data['Close'][i-1] < data['Close'][i-2]:\n",
        "            bullish_blocks.append((data.index[i-1], data['Close'][i-1]))\n",
        "        elif data['Close'][i] < data['Close'][i-1] and data['Close'][i-1] > data['Close'][i-2]:\n",
        "            bearish_blocks.append((data.index[i-1], data['Close'][i-1]))\n",
        "    return bullish_blocks, bearish_blocks\n",
        "\n",
        "# حساب Premium وDiscount\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def calculate_premium_discount(data):\n",
        "    \"\"\"حساب Premium وDiscount\"\"\"\n",
        "    # تحويل الأعمدة 'Close', 'Low', 'High' إلى نوع بيانات float64\n",
        "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
        "    data['Low'] = pd.to_numeric(data['Low'], errors='coerce')\n",
        "    data['High'] = pd.to_numeric(data['High'], errors='coerce')\n",
        "\n",
        "    # إزالة أي صفوف بها قيم مفقودة (NaN) بعد التحويل\n",
        "    data.dropna(subset=['Close', 'Low', 'High'], inplace=True)\n",
        "\n",
        "    premium = data['Close'] - data['Low'].rolling(window=20).min()\n",
        "    discount = data['High'].rolling(window=20).max() - data['Close']\n",
        "    return premium, discount\n",
        "\n",
        "\n",
        "\n",
        "# حساب مناطق العرض والطلب\n",
        "def calculate_supply_demand(data):\n",
        "    \"\"\"حساب مناطق العرض والطلب\"\"\"\n",
        "    supply_zones = []\n",
        "    demand_zones = []\n",
        "    for i in range(1, len(data)):\n",
        "        if data['Close'][i] > data['High'][i-1]:\n",
        "            supply_zones.append((data.index[i], data['High'][i-1]))\n",
        "        elif data['Close'][i] < data['Low'][i-1]:\n",
        "            demand_zones.append((data.index[i], data['Low'][i-1]))\n",
        "    return supply_zones, demand_zones\n",
        "\n",
        "# حساب خطوط الدعم والمقاومة\n",
        "def calculate_support_resistance(data):\n",
        "    \"\"\"حساب خطوط الدعم والمقاومة\"\"\"\n",
        "    support = data['Low'].rolling(window=20).min()\n",
        "    resistance = data['High'].rolling(window=20).max()\n",
        "    return support, resistance\n",
        "\n",
        "# حساب مستويات فيبوناتشي\n",
        "def fibonacci_retracement(high, low):\n",
        "    \"\"\"حساب مستويات فيبوناتشي\"\"\"\n",
        "    diff = high - low\n",
        "    return {\n",
        "        'fib_retracement_0.0': low,\n",
        "        'fib_retracement_23.6': low + diff * 0.236,\n",
        "        'fib_retracement_38.2': low + diff * 0.382,\n",
        "        'fib_retracement_50.0': low + diff * 0.5,\n",
        "        'fib_retracement_61.8': low + diff * 0.618,\n",
        "        'fib_retracement_100.0': high\n",
        "    }\n",
        "\n",
        "# حساب مؤشر القوة النسبية (RSI) باستخدام Pandas TA\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"حساب RSI\"\"\"\n",
        "    data['RSI'] = ta.rsi(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب المتوسط المتحرك باستخدام Pandas TA\n",
        "def calculate_moving_average(data, period=20):\n",
        "    \"\"\"حساب المتوسط المتحرك\"\"\"\n",
        "    data['MA'] = ta.sma(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# حساب الزخم (Momentum) باستخدام Pandas TA\n",
        "def calculate_momentum(data, period=10):\n",
        "    \"\"\"حساب الزخم\"\"\"\n",
        "    data['Momentum'] = ta.mom(data['Close'], length=period)\n",
        "    return data\n",
        "\n",
        "# تحليل حجم التداول\n",
        "def analyze_volume(data):\n",
        "    \"\"\"تحليل حجم التداول لتعزيز القمم والقيعان\"\"\"\n",
        "    data['Volume_MA'] = data['Volume'].rolling(window=20).mean()\n",
        "    return data\n",
        "\n",
        "# حساب موجة الذئب\n",
        "def calculate_wolf_wave(data):\n",
        "    \"\"\"حساب موجة الذئب\"\"\"\n",
        "    wave_points = []\n",
        "    for i in range(1, len(data)-4):\n",
        "        if (data['Close'][i] < data['Close'][i-1] and\n",
        "            data['Close'][i+1] < data['Close'][i] and\n",
        "            data['Close'][i+2] > data['Close'][i+1] and\n",
        "            data['Close'][i+3] < data['Close'][i+2] and\n",
        "            data['Close'][i+4] > data['Close'][i+3]):\n",
        "            wave_points.append(data.index[i])\n",
        "    return wave_points\n",
        "\n",
        "# إنشاء ميزات إضافية\n",
        "def create_features(data):\n",
        "    \"\"\"إنشاء ميزات إضافية مثل التغيرات في السعر\"\"\"\n",
        "    data['Price_Change'] = data['Close'].pct_change()\n",
        "    data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
        "    data['Volatility'] = data['Log_Returns'].rolling(window=20).std()\n",
        "    return data\n",
        "\n",
        "# حساب مؤشر القنوات السعرية\n",
        "def calculate_price_channel(data, window=20):\n",
        "    \"\"\"حساب مؤشر القنوات السعرية\"\"\"\n",
        "    data['Channel_Upper'] = data['High'].rolling(window=window).max()\n",
        "    data['Channel_Lower'] = data['Low'].rolling(window=window).min()\n",
        "    data['Channel_Mid'] = (data['Channel_Upper'] + data['Channel_Lower']) / 2\n",
        "    return data\n",
        "\n",
        "# تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\n",
        "def advanced_risk_management(data):\n",
        "    \"\"\"تحسين إدارة المخاطر باستخدام استراتيجيات التحليل الفني المتقدم\"\"\"\n",
        "    # حساب ATR (Average True Range)\n",
        "    data['ATR'] = ta.atr(data['High'], data['Low'], data['Close'], length=14)\n",
        "\n",
        "    # تحديد مستويات وقف الخسارة\n",
        "    data['Stop_Loss'] = data['Close'] - (data['ATR'] * 1.5)  # وقف خسارة عند 1.5 من ATR\n",
        "    data['Take_Profit'] = data['Close'] + (data['ATR'] * 3)    # جني الأرباح عند 3 من ATR\n",
        "\n",
        "    return data\n",
        "\n",
        "# دالة لحساب ورسم مناطق طلبات الشراء والبيع\n",
        "def احسب_وارسم_مناطق_طلبات_الشراء_والبيع(البيانات):\n",
        "    \"\"\"\n",
        "    يحسب ويرسم مناطق طلبات الشراء والبيع (Order Blocks) على الرسم البياني.\n",
        "\n",
        "    المعطيات:\n",
        "        البيانات: DataFrame بيانات السعر.\n",
        "    \"\"\"\n",
        "\n",
        "    # حساب مناطق طلبات الشراء\n",
        "    مناطق_طلبات_الشراء = []\n",
        "    for i in range(2, len(البيانات)):\n",
        "        if البيانات['Close'].iloc[i] < البيانات['Close'].iloc[i-1] and البيانات['Close'].iloc[i-1] > البيانات['Close'].iloc[i-2]:\n",
        "            مناطق_طلبات_الشراء.append((البيانات.index[i-1], البيانات['Close'].iloc[i-1]))\n",
        "\n",
        "    # حساب مناطق طلبات البيع\n",
        "    مناطق_طلبات_البيع = []\n",
        "    for i in range(2, len(البيانات)):\n",
        "        if البيانات['Close'].iloc[i] > البيانات['Close'].iloc[i-1] and البيانات['Close'].iloc[i-1] < البيانات['Close'].iloc[i-2]:\n",
        "            مناطق_طلبات_البيع.append((البيانات.index[i-1], البيانات['Close'].iloc[i-1]))\n",
        "\n",
        "    # رسم الرسم البياني\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(البيانات.index, البيانات['Close'], label='سعر الإغلاق', color='blue')\n",
        "\n",
        "    # رسم مناطق طلبات الشراء\n",
        "    for منطقة in مناطق_طلبات_الشراء:\n",
        "        index, price = منطقة\n",
        "        plt.plot(index, price, marker='o', markersize=8, color='red', label='طلب شراء' if منطقة == مناطق_طلبات_الشراء[0] else \"\")\n",
        "\n",
        "    # رسم مناطق طلبات البيع\n",
        "    for منطقة in مناطق_طلبات_البيع:\n",
        "        index, price = منطقة\n",
        "        plt.plot(index, price, marker='o', markersize=8, color='green', label='طلب بيع' if منطقة == مناطق_طلبات_البيع[0] else \"\")\n",
        "\n",
        "    plt.title('مناطق طلبات الشراء والبيع على الرسم البياني')\n",
        "    plt.xlabel('التاريخ')\n",
        "    plt.ylabel('السعر')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# بناء الاستراتيجية\n",
        "\n",
        "def build_strategy(data):\n",
        "    \"\"\"بناء الاستراتيجية الكاملة\"\"\"\n",
        "    data = load_and_clean_data()  # تحميل البيانات\n",
        "\n",
        "    # تحويل الأعمدة 'Adj Close', 'Open', 'Volume' إلى نوع بيانات float64\n",
        "    data['Adj Close'] = pd.to_numeric(data['Adj Close'], errors='coerce')\n",
        "    data['Open'] = pd.to_numeric(data['Open'], errors='coerce')\n",
        "    data['Volume'] = pd.to_numeric(data['Volume'], errors='coerce')\n",
        "\n",
        "    # إزالة أي صفوف بها قيم مفقودة (NaN) بعد التحويل\n",
        "    data.dropna(subset=['Adj Close', 'Open', 'Volume'], inplace=True)\n",
        "\n",
        "    # ... (بقية كودك) ...\n",
        "\n",
        "    # حساب Order Blocks\n",
        "    bullish_blocks, bearish_blocks = calculate_order_blocks(data) # تم إصلاح المسافة البادئة\n",
        "\n",
        "    # Initialization of signals_df to an empty DataFrame\n",
        "    signals_df = pd.DataFrame() # Initialize signals_df here\n",
        "\n",
        "    # ... (Rest of the signal generation and processing code) ...\n",
        "\n",
        "    return signals_df # Now, signals_df is always defined\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # حساب Premium وDiscount\n",
        "    premium, discount = calculate_premium_discount(data)\n",
        "    data['Premium'] = premium\n",
        "    data['Discount'] = discount\n",
        "\n",
        "    # حساب مناطق العرض والطلب\n",
        "    supply_zones, demand_zones = calculate_supply_demand(data)\n",
        "\n",
        "    # حساب خطوط الدعم والمقاومة\n",
        "    support, resistance = calculate_support_resistance(data)\n",
        "    data['Support'] = support\n",
        "    data['Resistance'] = resistance\n",
        "\n",
        "    # حساب مستويات فيبوناتشي\n",
        "    fib_levels = fibonacci_retracement(data['High'].max(), data['Low'].min())\n",
        "    for key, value in fib_levels.items():\n",
        "        data[key] = value\n",
        "\n",
        "    # حساب RSI\n",
        "    data = calculate_rsi(data)\n",
        "\n",
        "    # حساب Moving Average\n",
        "    data = calculate_moving_average(data)\n",
        "\n",
        "    # حساب Momentum\n",
        "    data = calculate_momentum(data)\n",
        "\n",
        "    # تحليل حجم التداول\n",
        "    data = analyze_volume(data)\n",
        "\n",
        "    # حساب موجة الذئب\n",
        "    wave_points = calculate_wolf_wave(data)\n",
        "\n",
        "    # إنشاء ميزات إضافية\n",
        "    data = create_features(data)\n",
        "\n",
        "    # حساب مؤشر القنوات السعرية\n",
        "    data = calculate_price_channel(data)\n",
        "\n",
        "    # تحسين إدارة المخاطر\n",
        "    data = advanced_risk_management(data)\n",
        "\n",
        "    # عرض النتائج\n",
        "    print(\"Bullish Order Blocks:\", bullish_blocks)\n",
        "    print(\"Bearish Order Blocks:\", bearish_blocks)\n",
        "    print(\"Premium and Discount calculated.\")\n",
        "\n",
        "    # تحليل المشاعر\n",
        "    news_titles = fetch_news()\n",
        "    sentiments = analyze_sentiment(news_titles)\n",
        "    data['Sentiment'] = np.mean(sentiments)\n",
        "\n",
        "    # جمع البيانات من المفكرة الاقتصادية\n",
        "    economic_events = fetch_economic_calendar()\n",
        "    print(\"Economic Events:\", economic_events)\n",
        "\n",
        "    # رسم بياني لمناطق طلبات الشراء والبيع\n",
        "    احسب_وارسم_مناطق_طلبات_الشراء_والبيع(data)\n",
        "\n",
        "    # إعداد الميزات والهدف\n",
        "    features = data.drop(['target'], axis=1, errors='ignore')  # تأكد من وجود العمود المستهدف\n",
        "    target = data['target'] if 'target' in data else np.zeros(len(data))  # في حالة عدم وجود العمود المستهدف\n",
        "\n",
        "    # تقسيم البيانات\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
        "\n",
        "    # نموذج XGBoost\n",
        "    model_xgb = XGBRegressor()\n",
        "    model_xgb.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج Random Forest\n",
        "    model_rf = RandomForestRegressor()\n",
        "    model_rf.fit(X_train, y_train)\n",
        "\n",
        "    # نموذج SVM\n",
        "    model_svr = SVR()\n",
        "    model_svr.fit(X_train, y_train)\n",
        "\n",
        "    # توليد الإشارات\n",
        "    signals_xgb = model_xgb.predict(X_test)\n",
        "    signals_rf = model_rf.predict(X_test)\n",
        "    signals_svr = model_svr.predict(X_test)\n",
        "\n",
        "    # دمج الإشارات في DataFrame\n",
        "    signals_df = pd.DataFrame({\n",
        "        'XGBoost': signals_xgb,\n",
        "        'RandomForest': signals_rf,\n",
        "        'SVM': signals_svr\n",
        "    })\n",
        "\n",
        "    # حساب متوسط الإشارات\n",
        "    signals_df['Average_Signal'] = signals_df.mean(axis=1)\n",
        "\n",
        "    # عرض بعض النتائج\n",
        "    print(\"\\nGenerated Trading Signals:\")\n",
        "    print(signals_df.head())\n",
        "\n",
        "    # تقييم النماذج\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    print(\"XGBoost R^2 Score:\", model_xgb.score(X_test, y_test))\n",
        "    print(\"Random Forest R^2 Score:\", model_rf.score(X_test, y_test))\n",
        "    print(\"SVM R^2 Score:\", model_svr.score(X_test, y_test))\n",
        "\n",
        "    return signals_df\n",
        "\n",
        "\n",
        "# تنفيذ الاستراتيجية\n",
        "if __name__ == \"__main__\":\n",
        "    # تحميل البيانات وتنفيذ الاستراتيجية\n",
        "    data = load_and_clean_data()\n",
        "    if data is not None:\n",
        "        # تطبيق الدالة الجديدة هنا:\n",
        "        data = set_price_as_index_and_clean(data)\n",
        "        trading_signals = build_strategy(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "K70VF6xtsfOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}